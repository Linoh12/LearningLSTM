{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linoh12/LearningLSTM/blob/main/LSTM_Lincoln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEe7a52QOsje"
      },
      "source": [
        "#Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tr0PG0POOdf",
        "outputId": "b6f59fa2-79bd-4f6a-c5cd-ea360d9cd68a"
      },
      "outputs": [],
      "source": [
        "# !pip install scikit-learn-intelex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "F98-YnmNNsme",
        "outputId": "4738bb95-08c8-47c2-cb6b-2d237742ac43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# from sklearnex import patch_sklearn\n",
        "# patch_sklearn()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "torch.__version__\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ivjbP5KvPPwM",
        "outputId": "ee299e0d-970c-410e-98f4-36e36dd4059b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SenderID</th>\n",
              "      <th>ReceiverID</th>\n",
              "      <th>Receiver_XVelocity</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Pathloss</th>\n",
              "      <th>PropDelay</th>\n",
              "      <th>Distance</th>\n",
              "      <th>Sender_XVelocity</th>\n",
              "      <th>Sender_YVelocity</th>\n",
              "      <th>Receiver_YVelocity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>72.6660</td>\n",
              "      <td>1.400000e-07</td>\n",
              "      <td>42.107007</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>-4.000000e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>71.0602</td>\n",
              "      <td>1.170000e-07</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>73.6697</td>\n",
              "      <td>1.580000e-07</td>\n",
              "      <td>47.265209</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>71.8491</td>\n",
              "      <td>1.280000e-07</td>\n",
              "      <td>38.327536</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>75.8679</td>\n",
              "      <td>2.030000e-07</td>\n",
              "      <td>60.876925</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>-3.000000e-08</td>\n",
              "      <td>-4.000000e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>14</td>\n",
              "      <td>17</td>\n",
              "      <td>-4.959106e-01</td>\n",
              "      <td>81.240209</td>\n",
              "      <td>69.7883</td>\n",
              "      <td>1.010000e-07</td>\n",
              "      <td>30.232433</td>\n",
              "      <td>-4.959106e-01</td>\n",
              "      <td>2.479553e+00</td>\n",
              "      <td>2.479553e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>-4.959106e-01</td>\n",
              "      <td>81.240209</td>\n",
              "      <td>65.4033</td>\n",
              "      <td>6.100000e-08</td>\n",
              "      <td>18.248288</td>\n",
              "      <td>-4.959106e-01</td>\n",
              "      <td>2.479553e+00</td>\n",
              "      <td>2.479553e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>14</td>\n",
              "      <td>19</td>\n",
              "      <td>-5.000000e-08</td>\n",
              "      <td>81.240209</td>\n",
              "      <td>90.7695</td>\n",
              "      <td>1.128000e-06</td>\n",
              "      <td>338.479901</td>\n",
              "      <td>-4.959106e-01</td>\n",
              "      <td>2.479553e+00</td>\n",
              "      <td>-9.000000e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "      <td>-5.000000e-08</td>\n",
              "      <td>81.240209</td>\n",
              "      <td>90.0503</td>\n",
              "      <td>1.039000e-06</td>\n",
              "      <td>311.579745</td>\n",
              "      <td>-4.959106e-01</td>\n",
              "      <td>2.479553e+00</td>\n",
              "      <td>-9.000000e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>14</td>\n",
              "      <td>21</td>\n",
              "      <td>-5.000000e-08</td>\n",
              "      <td>81.240209</td>\n",
              "      <td>89.8821</td>\n",
              "      <td>1.019000e-06</td>\n",
              "      <td>305.603640</td>\n",
              "      <td>-4.959106e-01</td>\n",
              "      <td>2.479553e+00</td>\n",
              "      <td>-9.000000e-08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       SenderID  ReceiverID  Receiver_XVelocity  Timestamp  Pathloss  \\\n",
              "0             1           2       -3.000000e-08   0.000006   72.6660   \n",
              "1             1           3       -3.000000e-08   0.000006   71.0602   \n",
              "2             1           4       -3.000000e-08   0.000006   73.6697   \n",
              "3             1           5       -3.000000e-08   0.000006   71.8491   \n",
              "4             1           6       -3.000000e-08   0.000006   75.8679   \n",
              "...         ...         ...                 ...        ...       ...   \n",
              "49995        14          17       -4.959106e-01  81.240209   69.7883   \n",
              "49996        14          18       -4.959106e-01  81.240209   65.4033   \n",
              "49997        14          19       -5.000000e-08  81.240209   90.7695   \n",
              "49998        14          20       -5.000000e-08  81.240209   90.0503   \n",
              "49999        14          21       -5.000000e-08  81.240209   89.8821   \n",
              "\n",
              "          PropDelay    Distance  Sender_XVelocity  Sender_YVelocity  \\\n",
              "0      1.400000e-07   42.107007     -3.000000e-08     -3.000000e-08   \n",
              "1      1.170000e-07   35.000000     -3.000000e-08     -3.000000e-08   \n",
              "2      1.580000e-07   47.265209     -3.000000e-08     -3.000000e-08   \n",
              "3      1.280000e-07   38.327536     -3.000000e-08     -3.000000e-08   \n",
              "4      2.030000e-07   60.876925     -3.000000e-08     -3.000000e-08   \n",
              "...             ...         ...               ...               ...   \n",
              "49995  1.010000e-07   30.232433     -4.959106e-01      2.479553e+00   \n",
              "49996  6.100000e-08   18.248288     -4.959106e-01      2.479553e+00   \n",
              "49997  1.128000e-06  338.479901     -4.959106e-01      2.479553e+00   \n",
              "49998  1.039000e-06  311.579745     -4.959106e-01      2.479553e+00   \n",
              "49999  1.019000e-06  305.603640     -4.959106e-01      2.479553e+00   \n",
              "\n",
              "       Receiver_YVelocity  \n",
              "0           -4.000000e-08  \n",
              "1           -3.000000e-08  \n",
              "2           -3.000000e-08  \n",
              "3           -3.000000e-08  \n",
              "4           -4.000000e-08  \n",
              "...                   ...  \n",
              "49995        2.479553e+00  \n",
              "49996        2.479553e+00  \n",
              "49997       -9.000000e-08  \n",
              "49998       -9.000000e-08  \n",
              "49999       -9.000000e-08  \n",
              "\n",
              "[50000 rows x 10 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fp = 'raw_data.csv'\n",
        "data = pd.read_csv(fp)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "5iA6DODENbzg",
        "outputId": "f5bb30ed-d40b-40ae-e708-479b8ea71ed3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SenderID</th>\n",
              "      <th>ReceiverID</th>\n",
              "      <th>Receiver_XVelocity</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Pathloss</th>\n",
              "      <th>PropDelay</th>\n",
              "      <th>Distance</th>\n",
              "      <th>Sender_XVelocity</th>\n",
              "      <th>Sender_YVelocity</th>\n",
              "      <th>Receiver_YVelocity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>-1.272443</td>\n",
              "      <td>-1.232738</td>\n",
              "      <td>-1.232222</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>-1.427426</td>\n",
              "      <td>-1.264487</td>\n",
              "      <td>-1.264923</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>-1.175571</td>\n",
              "      <td>-1.207890</td>\n",
              "      <td>-1.208487</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>-1.351286</td>\n",
              "      <td>-1.249303</td>\n",
              "      <td>-1.249612</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>-0.963413</td>\n",
              "      <td>-1.145772</td>\n",
              "      <td>-1.145856</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>14</td>\n",
              "      <td>17</td>\n",
              "      <td>-0.539460</td>\n",
              "      <td>2.064079</td>\n",
              "      <td>-1.550183</td>\n",
              "      <td>-1.286574</td>\n",
              "      <td>-1.286861</td>\n",
              "      <td>-0.681885</td>\n",
              "      <td>0.091776</td>\n",
              "      <td>0.379528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>-0.539460</td>\n",
              "      <td>2.064079</td>\n",
              "      <td>-1.973399</td>\n",
              "      <td>-1.341790</td>\n",
              "      <td>-1.342003</td>\n",
              "      <td>-0.681885</td>\n",
              "      <td>0.091776</td>\n",
              "      <td>0.379528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>14</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>2.064079</td>\n",
              "      <td>0.474808</td>\n",
              "      <td>0.131111</td>\n",
              "      <td>0.131483</td>\n",
              "      <td>-0.681885</td>\n",
              "      <td>0.091776</td>\n",
              "      <td>-0.550458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>2.064079</td>\n",
              "      <td>0.405394</td>\n",
              "      <td>0.008254</td>\n",
              "      <td>0.007707</td>\n",
              "      <td>-0.681885</td>\n",
              "      <td>0.091776</td>\n",
              "      <td>-0.550458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>14</td>\n",
              "      <td>21</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>2.064079</td>\n",
              "      <td>0.389161</td>\n",
              "      <td>-0.019354</td>\n",
              "      <td>-0.019791</td>\n",
              "      <td>-0.681885</td>\n",
              "      <td>0.091776</td>\n",
              "      <td>-0.550458</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       SenderID  ReceiverID  Receiver_XVelocity  Timestamp  Pathloss  \\\n",
              "0             1           2           -0.415269  -2.109152 -1.272443   \n",
              "1             1           3           -0.415269  -2.109152 -1.427426   \n",
              "2             1           4           -0.415269  -2.109152 -1.175571   \n",
              "3             1           5           -0.415269  -2.109152 -1.351286   \n",
              "4             1           6           -0.415269  -2.109152 -0.963413   \n",
              "...         ...         ...                 ...        ...       ...   \n",
              "49995        14          17           -0.539460   2.064079 -1.550183   \n",
              "49996        14          18           -0.539460   2.064079 -1.973399   \n",
              "49997        14          19           -0.415269   2.064079  0.474808   \n",
              "49998        14          20           -0.415269   2.064079  0.405394   \n",
              "49999        14          21           -0.415269   2.064079  0.389161   \n",
              "\n",
              "       PropDelay  Distance  Sender_XVelocity  Sender_YVelocity  \\\n",
              "0      -1.232738 -1.232222         -0.575348         -0.725865   \n",
              "1      -1.264487 -1.264923         -0.575348         -0.725865   \n",
              "2      -1.207890 -1.208487         -0.575348         -0.725865   \n",
              "3      -1.249303 -1.249612         -0.575348         -0.725865   \n",
              "4      -1.145772 -1.145856         -0.575348         -0.725865   \n",
              "...          ...       ...               ...               ...   \n",
              "49995  -1.286574 -1.286861         -0.681885          0.091776   \n",
              "49996  -1.341790 -1.342003         -0.681885          0.091776   \n",
              "49997   0.131111  0.131483         -0.681885          0.091776   \n",
              "49998   0.008254  0.007707         -0.681885          0.091776   \n",
              "49999  -0.019354 -0.019791         -0.681885          0.091776   \n",
              "\n",
              "       Receiver_YVelocity  \n",
              "0               -0.550458  \n",
              "1               -0.550458  \n",
              "2               -0.550458  \n",
              "3               -0.550458  \n",
              "4               -0.550458  \n",
              "...                   ...  \n",
              "49995            0.379528  \n",
              "49996            0.379528  \n",
              "49997           -0.550458  \n",
              "49998           -0.550458  \n",
              "49999           -0.550458  \n",
              "\n",
              "[50000 rows x 10 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Applying Standard Scalar to normalize data\n",
        "ID_features = ['SenderID','ReceiverID']\n",
        "scaled_data = pd.DataFrame(StandardScaler().fit_transform(data.drop(ID_features,axis=1)), columns=data.drop(ID_features,axis=1).columns)\n",
        "scaled_data = pd.concat((data[ID_features], scaled_data), axis=1)\n",
        "scaled_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTh-ZSeRpEKd",
        "outputId": "a56cf723-36d3-45f8-878c-b3110657dba8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{1, 2},\n",
              " {1, 3},\n",
              " {1, 4},\n",
              " {1, 5},\n",
              " {1, 6},\n",
              " {1, 7},\n",
              " {1, 9},\n",
              " {1, 10},\n",
              " {1, 11},\n",
              " {1, 12},\n",
              " {1, 13},\n",
              " {1, 14},\n",
              " {1, 15},\n",
              " {1, 17},\n",
              " {1, 18},\n",
              " {1, 19},\n",
              " {1, 20},\n",
              " {1, 21},\n",
              " {1, 22},\n",
              " {1, 23},\n",
              " {1, 25},\n",
              " {1, 26},\n",
              " {2, 3},\n",
              " {2, 4},\n",
              " {2, 5},\n",
              " {2, 6},\n",
              " {2, 7},\n",
              " {2, 10},\n",
              " {2, 11},\n",
              " {2, 12},\n",
              " {2, 13},\n",
              " {2, 14},\n",
              " {2, 15},\n",
              " {2, 18},\n",
              " {2, 19},\n",
              " {2, 20},\n",
              " {2, 21},\n",
              " {2, 22},\n",
              " {2, 23},\n",
              " {2, 26},\n",
              " {3, 4},\n",
              " {3, 5},\n",
              " {3, 6},\n",
              " {3, 7},\n",
              " {3, 12},\n",
              " {3, 13},\n",
              " {3, 14},\n",
              " {3, 15},\n",
              " {3, 20},\n",
              " {3, 21},\n",
              " {3, 22},\n",
              " {3, 23},\n",
              " {4, 5},\n",
              " {4, 6},\n",
              " {4, 7},\n",
              " {4, 12},\n",
              " {4, 13},\n",
              " {4, 14},\n",
              " {4, 15},\n",
              " {4, 20},\n",
              " {4, 21},\n",
              " {4, 22},\n",
              " {4, 23},\n",
              " {5, 6},\n",
              " {5, 7},\n",
              " {5, 14},\n",
              " {5, 15},\n",
              " {5, 22},\n",
              " {5, 23},\n",
              " {6, 7},\n",
              " {6, 14},\n",
              " {6, 15},\n",
              " {6, 22},\n",
              " {6, 23},\n",
              " {1, 8},\n",
              " {2, 8},\n",
              " {3, 8},\n",
              " {4, 8},\n",
              " {5, 8},\n",
              " {6, 8},\n",
              " {7, 8},\n",
              " {8, 9},\n",
              " {8, 10},\n",
              " {8, 11},\n",
              " {8, 12},\n",
              " {8, 13},\n",
              " {8, 14},\n",
              " {8, 15},\n",
              " {8, 16},\n",
              " {8, 17},\n",
              " {8, 18},\n",
              " {8, 19},\n",
              " {8, 20},\n",
              " {8, 21},\n",
              " {8, 22},\n",
              " {8, 23},\n",
              " {8, 24},\n",
              " {8, 25},\n",
              " {8, 26},\n",
              " {2, 9},\n",
              " {3, 9},\n",
              " {4, 9},\n",
              " {5, 9},\n",
              " {6, 9},\n",
              " {7, 9},\n",
              " {9, 10},\n",
              " {9, 11},\n",
              " {9, 12},\n",
              " {9, 13},\n",
              " {9, 14},\n",
              " {9, 15},\n",
              " {9, 17},\n",
              " {9, 18},\n",
              " {9, 19},\n",
              " {9, 20},\n",
              " {9, 21},\n",
              " {9, 22},\n",
              " {9, 23},\n",
              " {9, 25},\n",
              " {9, 26},\n",
              " {3, 10},\n",
              " {4, 10},\n",
              " {5, 10},\n",
              " {6, 10},\n",
              " {7, 10},\n",
              " {10, 11},\n",
              " {10, 12},\n",
              " {10, 13},\n",
              " {10, 14},\n",
              " {10, 15},\n",
              " {10, 18},\n",
              " {10, 19},\n",
              " {10, 20},\n",
              " {10, 21},\n",
              " {10, 22},\n",
              " {10, 23},\n",
              " {10, 26},\n",
              " {3, 11},\n",
              " {4, 11},\n",
              " {5, 11},\n",
              " {6, 11},\n",
              " {7, 11},\n",
              " {11, 12},\n",
              " {11, 13},\n",
              " {11, 14},\n",
              " {11, 15},\n",
              " {11, 20},\n",
              " {11, 21},\n",
              " {11, 22},\n",
              " {11, 23},\n",
              " {5, 12},\n",
              " {6, 12},\n",
              " {7, 12},\n",
              " {12, 13},\n",
              " {12, 14},\n",
              " {12, 15},\n",
              " {12, 20},\n",
              " {12, 21},\n",
              " {12, 22},\n",
              " {12, 23},\n",
              " {5, 13},\n",
              " {6, 13},\n",
              " {7, 13},\n",
              " {13, 14},\n",
              " {13, 15},\n",
              " {13, 22},\n",
              " {13, 23},\n",
              " {7, 14},\n",
              " {14, 15},\n",
              " {14, 22},\n",
              " {14, 23},\n",
              " {7, 15},\n",
              " {1, 16},\n",
              " {2, 16},\n",
              " {3, 16},\n",
              " {4, 16},\n",
              " {5, 16},\n",
              " {6, 16},\n",
              " {7, 16},\n",
              " {9, 16},\n",
              " {10, 16},\n",
              " {11, 16},\n",
              " {12, 16},\n",
              " {13, 16},\n",
              " {14, 16},\n",
              " {15, 16},\n",
              " {16, 17},\n",
              " {16, 18},\n",
              " {16, 19},\n",
              " {16, 20},\n",
              " {16, 21},\n",
              " {16, 22},\n",
              " {16, 23},\n",
              " {16, 24},\n",
              " {16, 25},\n",
              " {16, 26},\n",
              " {2, 17},\n",
              " {3, 17},\n",
              " {4, 17},\n",
              " {5, 17},\n",
              " {6, 17},\n",
              " {7, 17},\n",
              " {10, 17},\n",
              " {11, 17},\n",
              " {12, 17},\n",
              " {13, 17},\n",
              " {14, 17},\n",
              " {15, 17},\n",
              " {17, 18},\n",
              " {17, 19},\n",
              " {17, 20},\n",
              " {17, 21},\n",
              " {17, 22},\n",
              " {17, 23},\n",
              " {17, 25},\n",
              " {17, 26},\n",
              " {3, 18},\n",
              " {4, 18},\n",
              " {5, 18},\n",
              " {6, 18},\n",
              " {7, 18},\n",
              " {11, 18},\n",
              " {12, 18},\n",
              " {13, 18},\n",
              " {14, 18},\n",
              " {15, 18},\n",
              " {18, 19},\n",
              " {18, 20},\n",
              " {18, 21},\n",
              " {18, 22},\n",
              " {18, 23},\n",
              " {18, 26},\n",
              " {3, 19},\n",
              " {4, 19},\n",
              " {5, 19},\n",
              " {6, 19},\n",
              " {7, 19},\n",
              " {11, 19},\n",
              " {12, 19},\n",
              " {13, 19},\n",
              " {14, 19},\n",
              " {15, 19},\n",
              " {19, 20},\n",
              " {19, 21},\n",
              " {19, 22},\n",
              " {19, 23},\n",
              " {5, 20},\n",
              " {6, 20},\n",
              " {7, 20},\n",
              " {13, 20},\n",
              " {14, 20},\n",
              " {15, 20},\n",
              " {20, 21},\n",
              " {20, 22},\n",
              " {20, 23},\n",
              " {5, 21},\n",
              " {6, 21},\n",
              " {7, 21},\n",
              " {13, 21},\n",
              " {14, 21},\n",
              " {15, 21},\n",
              " {21, 22},\n",
              " {21, 23},\n",
              " {7, 22},\n",
              " {15, 22},\n",
              " {22, 23},\n",
              " {7, 23},\n",
              " {15, 23},\n",
              " {1, 24},\n",
              " {2, 24},\n",
              " {3, 24},\n",
              " {4, 24},\n",
              " {5, 24},\n",
              " {6, 24},\n",
              " {7, 24},\n",
              " {9, 24},\n",
              " {10, 24},\n",
              " {11, 24},\n",
              " {12, 24},\n",
              " {13, 24},\n",
              " {14, 24},\n",
              " {15, 24},\n",
              " {17, 24},\n",
              " {18, 24},\n",
              " {19, 24},\n",
              " {20, 24},\n",
              " {21, 24},\n",
              " {22, 24},\n",
              " {23, 24},\n",
              " {24, 25},\n",
              " {24, 26},\n",
              " {2, 25},\n",
              " {3, 25},\n",
              " {4, 25},\n",
              " {5, 25},\n",
              " {6, 25},\n",
              " {7, 25},\n",
              " {10, 25},\n",
              " {11, 25},\n",
              " {12, 25},\n",
              " {13, 25},\n",
              " {14, 25},\n",
              " {15, 25},\n",
              " {18, 25},\n",
              " {19, 25},\n",
              " {20, 25},\n",
              " {21, 25},\n",
              " {22, 25},\n",
              " {23, 25},\n",
              " {25, 26},\n",
              " {3, 26},\n",
              " {4, 26},\n",
              " {5, 26},\n",
              " {6, 26},\n",
              " {7, 26},\n",
              " {11, 26},\n",
              " {12, 26},\n",
              " {13, 26},\n",
              " {14, 26},\n",
              " {15, 26},\n",
              " {19, 26},\n",
              " {20, 26},\n",
              " {21, 26},\n",
              " {22, 26},\n",
              " {23, 26}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "possible_pairs = pd.Series([{x+1,y+1} if x!=y else None for x in range(26) for y in range(26)]).dropna().drop_duplicates().values.tolist()\n",
        "possible_pairs.sort(key=lambda x: list(x)[0])\n",
        "possible_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HQosPHGU6ii",
        "outputId": "ef69bece-6986-4ecf-a697-5041b9b71d82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(1, 2),\n",
              " (1, 3),\n",
              " (1, 4),\n",
              " (1, 5),\n",
              " (1, 6),\n",
              " (1, 7),\n",
              " (1, 8),\n",
              " (1, 9),\n",
              " (1, 10),\n",
              " (1, 11),\n",
              " (1, 12),\n",
              " (1, 13),\n",
              " (1, 14),\n",
              " (1, 15),\n",
              " (1, 16),\n",
              " (1, 17),\n",
              " (1, 18),\n",
              " (1, 19),\n",
              " (1, 20),\n",
              " (1, 21),\n",
              " (1, 22),\n",
              " (1, 23),\n",
              " (1, 24),\n",
              " (1, 25),\n",
              " (1, 26),\n",
              " (2, 1),\n",
              " (2, 3),\n",
              " (2, 4),\n",
              " (2, 5),\n",
              " (2, 6),\n",
              " (2, 7),\n",
              " (2, 8),\n",
              " (2, 9),\n",
              " (2, 10),\n",
              " (2, 11),\n",
              " (2, 12),\n",
              " (2, 13),\n",
              " (2, 14),\n",
              " (2, 15),\n",
              " (2, 16),\n",
              " (2, 17),\n",
              " (2, 18),\n",
              " (2, 19),\n",
              " (2, 20),\n",
              " (2, 21),\n",
              " (2, 22),\n",
              " (2, 23),\n",
              " (2, 24),\n",
              " (2, 25),\n",
              " (2, 26),\n",
              " (3, 1),\n",
              " (3, 2),\n",
              " (3, 4),\n",
              " (3, 5),\n",
              " (3, 6),\n",
              " (3, 7),\n",
              " (3, 8),\n",
              " (3, 9),\n",
              " (3, 10),\n",
              " (3, 11),\n",
              " (3, 12),\n",
              " (3, 13),\n",
              " (3, 14),\n",
              " (3, 15),\n",
              " (3, 16),\n",
              " (3, 17),\n",
              " (3, 18),\n",
              " (3, 19),\n",
              " (3, 20),\n",
              " (3, 21),\n",
              " (3, 22),\n",
              " (3, 23),\n",
              " (3, 24),\n",
              " (3, 25),\n",
              " (3, 26),\n",
              " (4, 1),\n",
              " (4, 2),\n",
              " (4, 3),\n",
              " (4, 5),\n",
              " (4, 6),\n",
              " (4, 7),\n",
              " (4, 8),\n",
              " (4, 9),\n",
              " (4, 10),\n",
              " (4, 11),\n",
              " (4, 12),\n",
              " (4, 13),\n",
              " (4, 14),\n",
              " (4, 15),\n",
              " (4, 16),\n",
              " (4, 17),\n",
              " (4, 18),\n",
              " (4, 19),\n",
              " (4, 20),\n",
              " (4, 21),\n",
              " (4, 22),\n",
              " (4, 23),\n",
              " (4, 24),\n",
              " (4, 25),\n",
              " (4, 26),\n",
              " (5, 1),\n",
              " (5, 2),\n",
              " (5, 3),\n",
              " (5, 4),\n",
              " (5, 6),\n",
              " (5, 7),\n",
              " (5, 8),\n",
              " (5, 9),\n",
              " (5, 10),\n",
              " (5, 11),\n",
              " (5, 12),\n",
              " (5, 13),\n",
              " (5, 14),\n",
              " (5, 15),\n",
              " (5, 16),\n",
              " (5, 17),\n",
              " (5, 18),\n",
              " (5, 19),\n",
              " (5, 20),\n",
              " (5, 21),\n",
              " (5, 22),\n",
              " (5, 23),\n",
              " (5, 24),\n",
              " (5, 25),\n",
              " (5, 26),\n",
              " (6, 1),\n",
              " (6, 2),\n",
              " (6, 3),\n",
              " (6, 4),\n",
              " (6, 5),\n",
              " (6, 7),\n",
              " (6, 8),\n",
              " (6, 9),\n",
              " (6, 10),\n",
              " (6, 11),\n",
              " (6, 12),\n",
              " (6, 13),\n",
              " (6, 14),\n",
              " (6, 15),\n",
              " (6, 16),\n",
              " (6, 17),\n",
              " (6, 18),\n",
              " (6, 19),\n",
              " (6, 20),\n",
              " (6, 21),\n",
              " (6, 22),\n",
              " (6, 23),\n",
              " (6, 24),\n",
              " (6, 25),\n",
              " (6, 26),\n",
              " (7, 1),\n",
              " (7, 2),\n",
              " (7, 3),\n",
              " (7, 4),\n",
              " (7, 5),\n",
              " (7, 6),\n",
              " (7, 8),\n",
              " (7, 9),\n",
              " (7, 10),\n",
              " (7, 11),\n",
              " (7, 12),\n",
              " (7, 13),\n",
              " (7, 14),\n",
              " (7, 15),\n",
              " (7, 16),\n",
              " (7, 17),\n",
              " (7, 18),\n",
              " (7, 19),\n",
              " (7, 20),\n",
              " (7, 21),\n",
              " (7, 22),\n",
              " (7, 23),\n",
              " (7, 24),\n",
              " (7, 25),\n",
              " (7, 26),\n",
              " (8, 1),\n",
              " (8, 2),\n",
              " (8, 3),\n",
              " (8, 4),\n",
              " (8, 5),\n",
              " (8, 6),\n",
              " (8, 7),\n",
              " (8, 9),\n",
              " (8, 10),\n",
              " (8, 11),\n",
              " (8, 12),\n",
              " (8, 13),\n",
              " (8, 14),\n",
              " (8, 15),\n",
              " (8, 16),\n",
              " (8, 17),\n",
              " (8, 18),\n",
              " (8, 19),\n",
              " (8, 20),\n",
              " (8, 21),\n",
              " (8, 22),\n",
              " (8, 23),\n",
              " (8, 24),\n",
              " (8, 25),\n",
              " (8, 26),\n",
              " (9, 1),\n",
              " (9, 2),\n",
              " (9, 3),\n",
              " (9, 4),\n",
              " (9, 5),\n",
              " (9, 6),\n",
              " (9, 7),\n",
              " (9, 8),\n",
              " (9, 10),\n",
              " (9, 11),\n",
              " (9, 12),\n",
              " (9, 13),\n",
              " (9, 14),\n",
              " (9, 15),\n",
              " (9, 16),\n",
              " (9, 17),\n",
              " (9, 18),\n",
              " (9, 19),\n",
              " (9, 20),\n",
              " (9, 21),\n",
              " (9, 22),\n",
              " (9, 23),\n",
              " (9, 24),\n",
              " (9, 25),\n",
              " (9, 26),\n",
              " (10, 1),\n",
              " (10, 2),\n",
              " (10, 3),\n",
              " (10, 4),\n",
              " (10, 5),\n",
              " (10, 6),\n",
              " (10, 7),\n",
              " (10, 8),\n",
              " (10, 9),\n",
              " (10, 11),\n",
              " (10, 12),\n",
              " (10, 13),\n",
              " (10, 14),\n",
              " (10, 15),\n",
              " (10, 16),\n",
              " (10, 17),\n",
              " (10, 18),\n",
              " (10, 19),\n",
              " (10, 20),\n",
              " (10, 21),\n",
              " (10, 22),\n",
              " (10, 23),\n",
              " (10, 24),\n",
              " (10, 25),\n",
              " (10, 26),\n",
              " (11, 1),\n",
              " (11, 2),\n",
              " (11, 3),\n",
              " (11, 4),\n",
              " (11, 5),\n",
              " (11, 6),\n",
              " (11, 7),\n",
              " (11, 8),\n",
              " (11, 9),\n",
              " (11, 10),\n",
              " (11, 12),\n",
              " (11, 13),\n",
              " (11, 14),\n",
              " (11, 15),\n",
              " (11, 16),\n",
              " (11, 17),\n",
              " (11, 18),\n",
              " (11, 19),\n",
              " (11, 20),\n",
              " (11, 21),\n",
              " (11, 22),\n",
              " (11, 23),\n",
              " (11, 24),\n",
              " (11, 25),\n",
              " (11, 26),\n",
              " (12, 1),\n",
              " (12, 2),\n",
              " (12, 3),\n",
              " (12, 4),\n",
              " (12, 5),\n",
              " (12, 6),\n",
              " (12, 7),\n",
              " (12, 8),\n",
              " (12, 9),\n",
              " (12, 10),\n",
              " (12, 11),\n",
              " (12, 13),\n",
              " (12, 14),\n",
              " (12, 15),\n",
              " (12, 16),\n",
              " (12, 17),\n",
              " (12, 18),\n",
              " (12, 19),\n",
              " (12, 20),\n",
              " (12, 21),\n",
              " (12, 22),\n",
              " (12, 23),\n",
              " (12, 24),\n",
              " (12, 25),\n",
              " (12, 26),\n",
              " (13, 1),\n",
              " (13, 2),\n",
              " (13, 3),\n",
              " (13, 4),\n",
              " (13, 5),\n",
              " (13, 6),\n",
              " (13, 7),\n",
              " (13, 8),\n",
              " (13, 9),\n",
              " (13, 10),\n",
              " (13, 11),\n",
              " (13, 12),\n",
              " (13, 14),\n",
              " (13, 15),\n",
              " (13, 16),\n",
              " (13, 17),\n",
              " (13, 18),\n",
              " (13, 19),\n",
              " (13, 20),\n",
              " (13, 21),\n",
              " (13, 22),\n",
              " (13, 23),\n",
              " (13, 24),\n",
              " (13, 25),\n",
              " (13, 26),\n",
              " (14, 1),\n",
              " (14, 2),\n",
              " (14, 3),\n",
              " (14, 4),\n",
              " (14, 5),\n",
              " (14, 6),\n",
              " (14, 7),\n",
              " (14, 8),\n",
              " (14, 9),\n",
              " (14, 10),\n",
              " (14, 11),\n",
              " (14, 12),\n",
              " (14, 13),\n",
              " (14, 15),\n",
              " (14, 16),\n",
              " (14, 17),\n",
              " (14, 18),\n",
              " (14, 19),\n",
              " (14, 20),\n",
              " (14, 21),\n",
              " (14, 22),\n",
              " (14, 23),\n",
              " (14, 24),\n",
              " (14, 25),\n",
              " (14, 26),\n",
              " (15, 1),\n",
              " (15, 2),\n",
              " (15, 3),\n",
              " (15, 4),\n",
              " (15, 5),\n",
              " (15, 6),\n",
              " (15, 7),\n",
              " (15, 8),\n",
              " (15, 9),\n",
              " (15, 10),\n",
              " (15, 11),\n",
              " (15, 12),\n",
              " (15, 13),\n",
              " (15, 14),\n",
              " (15, 16),\n",
              " (15, 17),\n",
              " (15, 18),\n",
              " (15, 19),\n",
              " (15, 20),\n",
              " (15, 21),\n",
              " (15, 22),\n",
              " (15, 23),\n",
              " (15, 24),\n",
              " (15, 25),\n",
              " (15, 26),\n",
              " (16, 1),\n",
              " (16, 2),\n",
              " (16, 3),\n",
              " (16, 4),\n",
              " (16, 5),\n",
              " (16, 6),\n",
              " (16, 7),\n",
              " (16, 8),\n",
              " (16, 9),\n",
              " (16, 10),\n",
              " (16, 11),\n",
              " (16, 12),\n",
              " (16, 13),\n",
              " (16, 14),\n",
              " (16, 15),\n",
              " (16, 17),\n",
              " (16, 18),\n",
              " (16, 19),\n",
              " (16, 20),\n",
              " (16, 21),\n",
              " (16, 22),\n",
              " (16, 23),\n",
              " (16, 24),\n",
              " (16, 25),\n",
              " (16, 26),\n",
              " (17, 1),\n",
              " (17, 2),\n",
              " (17, 3),\n",
              " (17, 4),\n",
              " (17, 5),\n",
              " (17, 6),\n",
              " (17, 7),\n",
              " (17, 8),\n",
              " (17, 9),\n",
              " (17, 10),\n",
              " (17, 11),\n",
              " (17, 12),\n",
              " (17, 13),\n",
              " (17, 14),\n",
              " (17, 15),\n",
              " (17, 16),\n",
              " (17, 18),\n",
              " (17, 19),\n",
              " (17, 20),\n",
              " (17, 21),\n",
              " (17, 22),\n",
              " (17, 23),\n",
              " (17, 24),\n",
              " (17, 25),\n",
              " (17, 26),\n",
              " (18, 1),\n",
              " (18, 2),\n",
              " (18, 3),\n",
              " (18, 4),\n",
              " (18, 5),\n",
              " (18, 6),\n",
              " (18, 7),\n",
              " (18, 8),\n",
              " (18, 9),\n",
              " (18, 10),\n",
              " (18, 11),\n",
              " (18, 12),\n",
              " (18, 13),\n",
              " (18, 14),\n",
              " (18, 15),\n",
              " (18, 16),\n",
              " (18, 17),\n",
              " (18, 19),\n",
              " (18, 20),\n",
              " (18, 21),\n",
              " (18, 22),\n",
              " (18, 23),\n",
              " (18, 24),\n",
              " (18, 25),\n",
              " (18, 26),\n",
              " (19, 1),\n",
              " (19, 2),\n",
              " (19, 3),\n",
              " (19, 4),\n",
              " (19, 5),\n",
              " (19, 6),\n",
              " (19, 7),\n",
              " (19, 8),\n",
              " (19, 9),\n",
              " (19, 10),\n",
              " (19, 11),\n",
              " (19, 12),\n",
              " (19, 13),\n",
              " (19, 14),\n",
              " (19, 15),\n",
              " (19, 16),\n",
              " (19, 17),\n",
              " (19, 18),\n",
              " (19, 20),\n",
              " (19, 21),\n",
              " (19, 22),\n",
              " (19, 23),\n",
              " (19, 24),\n",
              " (19, 25),\n",
              " (19, 26),\n",
              " (20, 1),\n",
              " (20, 2),\n",
              " (20, 3),\n",
              " (20, 4),\n",
              " (20, 5),\n",
              " (20, 6),\n",
              " (20, 7),\n",
              " (20, 8),\n",
              " (20, 9),\n",
              " (20, 10),\n",
              " (20, 11),\n",
              " (20, 12),\n",
              " (20, 13),\n",
              " (20, 14),\n",
              " (20, 15),\n",
              " (20, 16),\n",
              " (20, 17),\n",
              " (20, 18),\n",
              " (20, 19),\n",
              " (20, 21),\n",
              " (20, 22),\n",
              " (20, 23),\n",
              " (20, 24),\n",
              " (20, 25),\n",
              " (20, 26),\n",
              " (21, 1),\n",
              " (21, 2),\n",
              " (21, 3),\n",
              " (21, 4),\n",
              " (21, 5),\n",
              " (21, 6),\n",
              " (21, 7),\n",
              " (21, 8),\n",
              " (21, 9),\n",
              " (21, 10),\n",
              " (21, 11),\n",
              " (21, 12),\n",
              " (21, 13),\n",
              " (21, 14),\n",
              " (21, 15),\n",
              " (21, 16),\n",
              " (21, 17),\n",
              " (21, 18),\n",
              " (21, 19),\n",
              " (21, 20),\n",
              " (21, 22),\n",
              " (21, 23),\n",
              " (21, 24),\n",
              " (21, 25),\n",
              " (21, 26),\n",
              " (22, 1),\n",
              " (22, 2),\n",
              " (22, 3),\n",
              " (22, 4),\n",
              " (22, 5),\n",
              " (22, 6),\n",
              " (22, 7),\n",
              " (22, 8),\n",
              " (22, 9),\n",
              " (22, 10),\n",
              " (22, 11),\n",
              " (22, 12),\n",
              " (22, 13),\n",
              " (22, 14),\n",
              " (22, 15),\n",
              " (22, 16),\n",
              " (22, 17),\n",
              " (22, 18),\n",
              " (22, 19),\n",
              " (22, 20),\n",
              " (22, 21),\n",
              " (22, 23),\n",
              " (22, 24),\n",
              " (22, 25),\n",
              " (22, 26),\n",
              " (23, 1),\n",
              " (23, 2),\n",
              " (23, 3),\n",
              " (23, 4),\n",
              " (23, 5),\n",
              " (23, 6),\n",
              " (23, 7),\n",
              " (23, 8),\n",
              " (23, 9),\n",
              " (23, 10),\n",
              " (23, 11),\n",
              " (23, 12),\n",
              " (23, 13),\n",
              " (23, 14),\n",
              " (23, 15),\n",
              " (23, 16),\n",
              " (23, 17),\n",
              " (23, 18),\n",
              " (23, 19),\n",
              " (23, 20),\n",
              " (23, 21),\n",
              " (23, 22),\n",
              " (23, 24),\n",
              " (23, 25),\n",
              " (23, 26),\n",
              " (24, 1),\n",
              " (24, 2),\n",
              " (24, 3),\n",
              " (24, 4),\n",
              " (24, 5),\n",
              " (24, 6),\n",
              " (24, 7),\n",
              " (24, 8),\n",
              " (24, 9),\n",
              " (24, 10),\n",
              " (24, 11),\n",
              " (24, 12),\n",
              " (24, 13),\n",
              " (24, 14),\n",
              " (24, 15),\n",
              " (24, 16),\n",
              " (24, 17),\n",
              " (24, 18),\n",
              " (24, 19),\n",
              " (24, 20),\n",
              " (24, 21),\n",
              " (24, 22),\n",
              " (24, 23),\n",
              " (24, 25),\n",
              " (24, 26),\n",
              " (25, 1),\n",
              " (25, 2),\n",
              " (25, 3),\n",
              " (25, 4),\n",
              " (25, 5),\n",
              " (25, 6),\n",
              " (25, 7),\n",
              " (25, 8),\n",
              " (25, 9),\n",
              " (25, 10),\n",
              " (25, 11),\n",
              " (25, 12),\n",
              " (25, 13),\n",
              " (25, 14),\n",
              " (25, 15),\n",
              " (25, 16),\n",
              " (25, 17),\n",
              " (25, 18),\n",
              " (25, 19),\n",
              " (25, 20),\n",
              " (25, 21),\n",
              " (25, 22),\n",
              " (25, 23),\n",
              " (25, 24),\n",
              " (25, 26),\n",
              " (26, 1),\n",
              " (26, 2),\n",
              " (26, 3),\n",
              " (26, 4),\n",
              " (26, 5),\n",
              " (26, 6),\n",
              " (26, 7),\n",
              " (26, 8),\n",
              " (26, 9),\n",
              " (26, 10),\n",
              " (26, 11),\n",
              " (26, 12),\n",
              " (26, 13),\n",
              " (26, 14),\n",
              " (26, 15),\n",
              " (26, 16),\n",
              " (26, 17),\n",
              " (26, 18),\n",
              " (26, 19),\n",
              " (26, 20),\n",
              " (26, 21),\n",
              " (26, 22),\n",
              " (26, 23),\n",
              " (26, 24),\n",
              " (26, 25)]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_pairs = pd.Series([(x+1,y+1) if x!=y else None for x in range(26) for y in range(26)]).dropna().drop_duplicates().values.tolist()\n",
        "all_pairs.sort(key=lambda x: list(x)[0])\n",
        "all_pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-us6V1AbUreU",
        "outputId": "22919535-0cbc-410e-9eca-e66f30933845"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{(1, 2): 0,\n",
              " (1, 3): 1,\n",
              " (1, 4): 2,\n",
              " (1, 5): 3,\n",
              " (1, 6): 4,\n",
              " (1, 7): 5,\n",
              " (1, 8): 6,\n",
              " (1, 9): 7,\n",
              " (1, 10): 8,\n",
              " (1, 11): 9,\n",
              " (1, 12): 10,\n",
              " (1, 13): 11,\n",
              " (1, 14): 12,\n",
              " (1, 15): 13,\n",
              " (1, 16): 14,\n",
              " (1, 17): 15,\n",
              " (1, 18): 16,\n",
              " (1, 19): 17,\n",
              " (1, 20): 18,\n",
              " (1, 21): 19,\n",
              " (1, 22): 20,\n",
              " (1, 23): 21,\n",
              " (1, 24): 22,\n",
              " (1, 25): 23,\n",
              " (1, 26): 24,\n",
              " (2, 1): 25,\n",
              " (2, 3): 26,\n",
              " (2, 4): 27,\n",
              " (2, 5): 28,\n",
              " (2, 6): 29,\n",
              " (2, 7): 30,\n",
              " (2, 8): 31,\n",
              " (2, 9): 32,\n",
              " (2, 10): 33,\n",
              " (2, 11): 34,\n",
              " (2, 12): 35,\n",
              " (2, 13): 36,\n",
              " (2, 14): 37,\n",
              " (2, 15): 38,\n",
              " (2, 16): 39,\n",
              " (2, 17): 40,\n",
              " (2, 18): 41,\n",
              " (2, 19): 42,\n",
              " (2, 20): 43,\n",
              " (2, 21): 44,\n",
              " (2, 22): 45,\n",
              " (2, 23): 46,\n",
              " (2, 24): 47,\n",
              " (2, 25): 48,\n",
              " (2, 26): 49,\n",
              " (3, 1): 50,\n",
              " (3, 2): 51,\n",
              " (3, 4): 52,\n",
              " (3, 5): 53,\n",
              " (3, 6): 54,\n",
              " (3, 7): 55,\n",
              " (3, 8): 56,\n",
              " (3, 9): 57,\n",
              " (3, 10): 58,\n",
              " (3, 11): 59,\n",
              " (3, 12): 60,\n",
              " (3, 13): 61,\n",
              " (3, 14): 62,\n",
              " (3, 15): 63,\n",
              " (3, 16): 64,\n",
              " (3, 17): 65,\n",
              " (3, 18): 66,\n",
              " (3, 19): 67,\n",
              " (3, 20): 68,\n",
              " (3, 21): 69,\n",
              " (3, 22): 70,\n",
              " (3, 23): 71,\n",
              " (3, 24): 72,\n",
              " (3, 25): 73,\n",
              " (3, 26): 74,\n",
              " (4, 1): 75,\n",
              " (4, 2): 76,\n",
              " (4, 3): 77,\n",
              " (4, 5): 78,\n",
              " (4, 6): 79,\n",
              " (4, 7): 80,\n",
              " (4, 8): 81,\n",
              " (4, 9): 82,\n",
              " (4, 10): 83,\n",
              " (4, 11): 84,\n",
              " (4, 12): 85,\n",
              " (4, 13): 86,\n",
              " (4, 14): 87,\n",
              " (4, 15): 88,\n",
              " (4, 16): 89,\n",
              " (4, 17): 90,\n",
              " (4, 18): 91,\n",
              " (4, 19): 92,\n",
              " (4, 20): 93,\n",
              " (4, 21): 94,\n",
              " (4, 22): 95,\n",
              " (4, 23): 96,\n",
              " (4, 24): 97,\n",
              " (4, 25): 98,\n",
              " (4, 26): 99,\n",
              " (5, 1): 100,\n",
              " (5, 2): 101,\n",
              " (5, 3): 102,\n",
              " (5, 4): 103,\n",
              " (5, 6): 104,\n",
              " (5, 7): 105,\n",
              " (5, 8): 106,\n",
              " (5, 9): 107,\n",
              " (5, 10): 108,\n",
              " (5, 11): 109,\n",
              " (5, 12): 110,\n",
              " (5, 13): 111,\n",
              " (5, 14): 112,\n",
              " (5, 15): 113,\n",
              " (5, 16): 114,\n",
              " (5, 17): 115,\n",
              " (5, 18): 116,\n",
              " (5, 19): 117,\n",
              " (5, 20): 118,\n",
              " (5, 21): 119,\n",
              " (5, 22): 120,\n",
              " (5, 23): 121,\n",
              " (5, 24): 122,\n",
              " (5, 25): 123,\n",
              " (5, 26): 124,\n",
              " (6, 1): 125,\n",
              " (6, 2): 126,\n",
              " (6, 3): 127,\n",
              " (6, 4): 128,\n",
              " (6, 5): 129,\n",
              " (6, 7): 130,\n",
              " (6, 8): 131,\n",
              " (6, 9): 132,\n",
              " (6, 10): 133,\n",
              " (6, 11): 134,\n",
              " (6, 12): 135,\n",
              " (6, 13): 136,\n",
              " (6, 14): 137,\n",
              " (6, 15): 138,\n",
              " (6, 16): 139,\n",
              " (6, 17): 140,\n",
              " (6, 18): 141,\n",
              " (6, 19): 142,\n",
              " (6, 20): 143,\n",
              " (6, 21): 144,\n",
              " (6, 22): 145,\n",
              " (6, 23): 146,\n",
              " (6, 24): 147,\n",
              " (6, 25): 148,\n",
              " (6, 26): 149,\n",
              " (7, 1): 150,\n",
              " (7, 2): 151,\n",
              " (7, 3): 152,\n",
              " (7, 4): 153,\n",
              " (7, 5): 154,\n",
              " (7, 6): 155,\n",
              " (7, 8): 156,\n",
              " (7, 9): 157,\n",
              " (7, 10): 158,\n",
              " (7, 11): 159,\n",
              " (7, 12): 160,\n",
              " (7, 13): 161,\n",
              " (7, 14): 162,\n",
              " (7, 15): 163,\n",
              " (7, 16): 164,\n",
              " (7, 17): 165,\n",
              " (7, 18): 166,\n",
              " (7, 19): 167,\n",
              " (7, 20): 168,\n",
              " (7, 21): 169,\n",
              " (7, 22): 170,\n",
              " (7, 23): 171,\n",
              " (7, 24): 172,\n",
              " (7, 25): 173,\n",
              " (7, 26): 174,\n",
              " (8, 1): 175,\n",
              " (8, 2): 176,\n",
              " (8, 3): 177,\n",
              " (8, 4): 178,\n",
              " (8, 5): 179,\n",
              " (8, 6): 180,\n",
              " (8, 7): 181,\n",
              " (8, 9): 182,\n",
              " (8, 10): 183,\n",
              " (8, 11): 184,\n",
              " (8, 12): 185,\n",
              " (8, 13): 186,\n",
              " (8, 14): 187,\n",
              " (8, 15): 188,\n",
              " (8, 16): 189,\n",
              " (8, 17): 190,\n",
              " (8, 18): 191,\n",
              " (8, 19): 192,\n",
              " (8, 20): 193,\n",
              " (8, 21): 194,\n",
              " (8, 22): 195,\n",
              " (8, 23): 196,\n",
              " (8, 24): 197,\n",
              " (8, 25): 198,\n",
              " (8, 26): 199,\n",
              " (9, 1): 200,\n",
              " (9, 2): 201,\n",
              " (9, 3): 202,\n",
              " (9, 4): 203,\n",
              " (9, 5): 204,\n",
              " (9, 6): 205,\n",
              " (9, 7): 206,\n",
              " (9, 8): 207,\n",
              " (9, 10): 208,\n",
              " (9, 11): 209,\n",
              " (9, 12): 210,\n",
              " (9, 13): 211,\n",
              " (9, 14): 212,\n",
              " (9, 15): 213,\n",
              " (9, 16): 214,\n",
              " (9, 17): 215,\n",
              " (9, 18): 216,\n",
              " (9, 19): 217,\n",
              " (9, 20): 218,\n",
              " (9, 21): 219,\n",
              " (9, 22): 220,\n",
              " (9, 23): 221,\n",
              " (9, 24): 222,\n",
              " (9, 25): 223,\n",
              " (9, 26): 224,\n",
              " (10, 1): 225,\n",
              " (10, 2): 226,\n",
              " (10, 3): 227,\n",
              " (10, 4): 228,\n",
              " (10, 5): 229,\n",
              " (10, 6): 230,\n",
              " (10, 7): 231,\n",
              " (10, 8): 232,\n",
              " (10, 9): 233,\n",
              " (10, 11): 234,\n",
              " (10, 12): 235,\n",
              " (10, 13): 236,\n",
              " (10, 14): 237,\n",
              " (10, 15): 238,\n",
              " (10, 16): 239,\n",
              " (10, 17): 240,\n",
              " (10, 18): 241,\n",
              " (10, 19): 242,\n",
              " (10, 20): 243,\n",
              " (10, 21): 244,\n",
              " (10, 22): 245,\n",
              " (10, 23): 246,\n",
              " (10, 24): 247,\n",
              " (10, 25): 248,\n",
              " (10, 26): 249,\n",
              " (11, 1): 250,\n",
              " (11, 2): 251,\n",
              " (11, 3): 252,\n",
              " (11, 4): 253,\n",
              " (11, 5): 254,\n",
              " (11, 6): 255,\n",
              " (11, 7): 256,\n",
              " (11, 8): 257,\n",
              " (11, 9): 258,\n",
              " (11, 10): 259,\n",
              " (11, 12): 260,\n",
              " (11, 13): 261,\n",
              " (11, 14): 262,\n",
              " (11, 15): 263,\n",
              " (11, 16): 264,\n",
              " (11, 17): 265,\n",
              " (11, 18): 266,\n",
              " (11, 19): 267,\n",
              " (11, 20): 268,\n",
              " (11, 21): 269,\n",
              " (11, 22): 270,\n",
              " (11, 23): 271,\n",
              " (11, 24): 272,\n",
              " (11, 25): 273,\n",
              " (11, 26): 274,\n",
              " (12, 1): 275,\n",
              " (12, 2): 276,\n",
              " (12, 3): 277,\n",
              " (12, 4): 278,\n",
              " (12, 5): 279,\n",
              " (12, 6): 280,\n",
              " (12, 7): 281,\n",
              " (12, 8): 282,\n",
              " (12, 9): 283,\n",
              " (12, 10): 284,\n",
              " (12, 11): 285,\n",
              " (12, 13): 286,\n",
              " (12, 14): 287,\n",
              " (12, 15): 288,\n",
              " (12, 16): 289,\n",
              " (12, 17): 290,\n",
              " (12, 18): 291,\n",
              " (12, 19): 292,\n",
              " (12, 20): 293,\n",
              " (12, 21): 294,\n",
              " (12, 22): 295,\n",
              " (12, 23): 296,\n",
              " (12, 24): 297,\n",
              " (12, 25): 298,\n",
              " (12, 26): 299,\n",
              " (13, 1): 300,\n",
              " (13, 2): 301,\n",
              " (13, 3): 302,\n",
              " (13, 4): 303,\n",
              " (13, 5): 304,\n",
              " (13, 6): 305,\n",
              " (13, 7): 306,\n",
              " (13, 8): 307,\n",
              " (13, 9): 308,\n",
              " (13, 10): 309,\n",
              " (13, 11): 310,\n",
              " (13, 12): 311,\n",
              " (13, 14): 312,\n",
              " (13, 15): 313,\n",
              " (13, 16): 314,\n",
              " (13, 17): 315,\n",
              " (13, 18): 316,\n",
              " (13, 19): 317,\n",
              " (13, 20): 318,\n",
              " (13, 21): 319,\n",
              " (13, 22): 320,\n",
              " (13, 23): 321,\n",
              " (13, 24): 322,\n",
              " (13, 25): 323,\n",
              " (13, 26): 324,\n",
              " (14, 1): 325,\n",
              " (14, 2): 326,\n",
              " (14, 3): 327,\n",
              " (14, 4): 328,\n",
              " (14, 5): 329,\n",
              " (14, 6): 330,\n",
              " (14, 7): 331,\n",
              " (14, 8): 332,\n",
              " (14, 9): 333,\n",
              " (14, 10): 334,\n",
              " (14, 11): 335,\n",
              " (14, 12): 336,\n",
              " (14, 13): 337,\n",
              " (14, 15): 338,\n",
              " (14, 16): 339,\n",
              " (14, 17): 340,\n",
              " (14, 18): 341,\n",
              " (14, 19): 342,\n",
              " (14, 20): 343,\n",
              " (14, 21): 344,\n",
              " (14, 22): 345,\n",
              " (14, 23): 346,\n",
              " (14, 24): 347,\n",
              " (14, 25): 348,\n",
              " (14, 26): 349,\n",
              " (15, 1): 350,\n",
              " (15, 2): 351,\n",
              " (15, 3): 352,\n",
              " (15, 4): 353,\n",
              " (15, 5): 354,\n",
              " (15, 6): 355,\n",
              " (15, 7): 356,\n",
              " (15, 8): 357,\n",
              " (15, 9): 358,\n",
              " (15, 10): 359,\n",
              " (15, 11): 360,\n",
              " (15, 12): 361,\n",
              " (15, 13): 362,\n",
              " (15, 14): 363,\n",
              " (15, 16): 364,\n",
              " (15, 17): 365,\n",
              " (15, 18): 366,\n",
              " (15, 19): 367,\n",
              " (15, 20): 368,\n",
              " (15, 21): 369,\n",
              " (15, 22): 370,\n",
              " (15, 23): 371,\n",
              " (15, 24): 372,\n",
              " (15, 25): 373,\n",
              " (15, 26): 374,\n",
              " (16, 1): 375,\n",
              " (16, 2): 376,\n",
              " (16, 3): 377,\n",
              " (16, 4): 378,\n",
              " (16, 5): 379,\n",
              " (16, 6): 380,\n",
              " (16, 7): 381,\n",
              " (16, 8): 382,\n",
              " (16, 9): 383,\n",
              " (16, 10): 384,\n",
              " (16, 11): 385,\n",
              " (16, 12): 386,\n",
              " (16, 13): 387,\n",
              " (16, 14): 388,\n",
              " (16, 15): 389,\n",
              " (16, 17): 390,\n",
              " (16, 18): 391,\n",
              " (16, 19): 392,\n",
              " (16, 20): 393,\n",
              " (16, 21): 394,\n",
              " (16, 22): 395,\n",
              " (16, 23): 396,\n",
              " (16, 24): 397,\n",
              " (16, 25): 398,\n",
              " (16, 26): 399,\n",
              " (17, 1): 400,\n",
              " (17, 2): 401,\n",
              " (17, 3): 402,\n",
              " (17, 4): 403,\n",
              " (17, 5): 404,\n",
              " (17, 6): 405,\n",
              " (17, 7): 406,\n",
              " (17, 8): 407,\n",
              " (17, 9): 408,\n",
              " (17, 10): 409,\n",
              " (17, 11): 410,\n",
              " (17, 12): 411,\n",
              " (17, 13): 412,\n",
              " (17, 14): 413,\n",
              " (17, 15): 414,\n",
              " (17, 16): 415,\n",
              " (17, 18): 416,\n",
              " (17, 19): 417,\n",
              " (17, 20): 418,\n",
              " (17, 21): 419,\n",
              " (17, 22): 420,\n",
              " (17, 23): 421,\n",
              " (17, 24): 422,\n",
              " (17, 25): 423,\n",
              " (17, 26): 424,\n",
              " (18, 1): 425,\n",
              " (18, 2): 426,\n",
              " (18, 3): 427,\n",
              " (18, 4): 428,\n",
              " (18, 5): 429,\n",
              " (18, 6): 430,\n",
              " (18, 7): 431,\n",
              " (18, 8): 432,\n",
              " (18, 9): 433,\n",
              " (18, 10): 434,\n",
              " (18, 11): 435,\n",
              " (18, 12): 436,\n",
              " (18, 13): 437,\n",
              " (18, 14): 438,\n",
              " (18, 15): 439,\n",
              " (18, 16): 440,\n",
              " (18, 17): 441,\n",
              " (18, 19): 442,\n",
              " (18, 20): 443,\n",
              " (18, 21): 444,\n",
              " (18, 22): 445,\n",
              " (18, 23): 446,\n",
              " (18, 24): 447,\n",
              " (18, 25): 448,\n",
              " (18, 26): 449,\n",
              " (19, 1): 450,\n",
              " (19, 2): 451,\n",
              " (19, 3): 452,\n",
              " (19, 4): 453,\n",
              " (19, 5): 454,\n",
              " (19, 6): 455,\n",
              " (19, 7): 456,\n",
              " (19, 8): 457,\n",
              " (19, 9): 458,\n",
              " (19, 10): 459,\n",
              " (19, 11): 460,\n",
              " (19, 12): 461,\n",
              " (19, 13): 462,\n",
              " (19, 14): 463,\n",
              " (19, 15): 464,\n",
              " (19, 16): 465,\n",
              " (19, 17): 466,\n",
              " (19, 18): 467,\n",
              " (19, 20): 468,\n",
              " (19, 21): 469,\n",
              " (19, 22): 470,\n",
              " (19, 23): 471,\n",
              " (19, 24): 472,\n",
              " (19, 25): 473,\n",
              " (19, 26): 474,\n",
              " (20, 1): 475,\n",
              " (20, 2): 476,\n",
              " (20, 3): 477,\n",
              " (20, 4): 478,\n",
              " (20, 5): 479,\n",
              " (20, 6): 480,\n",
              " (20, 7): 481,\n",
              " (20, 8): 482,\n",
              " (20, 9): 483,\n",
              " (20, 10): 484,\n",
              " (20, 11): 485,\n",
              " (20, 12): 486,\n",
              " (20, 13): 487,\n",
              " (20, 14): 488,\n",
              " (20, 15): 489,\n",
              " (20, 16): 490,\n",
              " (20, 17): 491,\n",
              " (20, 18): 492,\n",
              " (20, 19): 493,\n",
              " (20, 21): 494,\n",
              " (20, 22): 495,\n",
              " (20, 23): 496,\n",
              " (20, 24): 497,\n",
              " (20, 25): 498,\n",
              " (20, 26): 499,\n",
              " (21, 1): 500,\n",
              " (21, 2): 501,\n",
              " (21, 3): 502,\n",
              " (21, 4): 503,\n",
              " (21, 5): 504,\n",
              " (21, 6): 505,\n",
              " (21, 7): 506,\n",
              " (21, 8): 507,\n",
              " (21, 9): 508,\n",
              " (21, 10): 509,\n",
              " (21, 11): 510,\n",
              " (21, 12): 511,\n",
              " (21, 13): 512,\n",
              " (21, 14): 513,\n",
              " (21, 15): 514,\n",
              " (21, 16): 515,\n",
              " (21, 17): 516,\n",
              " (21, 18): 517,\n",
              " (21, 19): 518,\n",
              " (21, 20): 519,\n",
              " (21, 22): 520,\n",
              " (21, 23): 521,\n",
              " (21, 24): 522,\n",
              " (21, 25): 523,\n",
              " (21, 26): 524,\n",
              " (22, 1): 525,\n",
              " (22, 2): 526,\n",
              " (22, 3): 527,\n",
              " (22, 4): 528,\n",
              " (22, 5): 529,\n",
              " (22, 6): 530,\n",
              " (22, 7): 531,\n",
              " (22, 8): 532,\n",
              " (22, 9): 533,\n",
              " (22, 10): 534,\n",
              " (22, 11): 535,\n",
              " (22, 12): 536,\n",
              " (22, 13): 537,\n",
              " (22, 14): 538,\n",
              " (22, 15): 539,\n",
              " (22, 16): 540,\n",
              " (22, 17): 541,\n",
              " (22, 18): 542,\n",
              " (22, 19): 543,\n",
              " (22, 20): 544,\n",
              " (22, 21): 545,\n",
              " (22, 23): 546,\n",
              " (22, 24): 547,\n",
              " (22, 25): 548,\n",
              " (22, 26): 549,\n",
              " (23, 1): 550,\n",
              " (23, 2): 551,\n",
              " (23, 3): 552,\n",
              " (23, 4): 553,\n",
              " (23, 5): 554,\n",
              " (23, 6): 555,\n",
              " (23, 7): 556,\n",
              " (23, 8): 557,\n",
              " (23, 9): 558,\n",
              " (23, 10): 559,\n",
              " (23, 11): 560,\n",
              " (23, 12): 561,\n",
              " (23, 13): 562,\n",
              " (23, 14): 563,\n",
              " (23, 15): 564,\n",
              " (23, 16): 565,\n",
              " (23, 17): 566,\n",
              " (23, 18): 567,\n",
              " (23, 19): 568,\n",
              " (23, 20): 569,\n",
              " (23, 21): 570,\n",
              " (23, 22): 571,\n",
              " (23, 24): 572,\n",
              " (23, 25): 573,\n",
              " (23, 26): 574,\n",
              " (24, 1): 575,\n",
              " (24, 2): 576,\n",
              " (24, 3): 577,\n",
              " (24, 4): 578,\n",
              " (24, 5): 579,\n",
              " (24, 6): 580,\n",
              " (24, 7): 581,\n",
              " (24, 8): 582,\n",
              " (24, 9): 583,\n",
              " (24, 10): 584,\n",
              " (24, 11): 585,\n",
              " (24, 12): 586,\n",
              " (24, 13): 587,\n",
              " (24, 14): 588,\n",
              " (24, 15): 589,\n",
              " (24, 16): 590,\n",
              " (24, 17): 591,\n",
              " (24, 18): 592,\n",
              " (24, 19): 593,\n",
              " (24, 20): 594,\n",
              " (24, 21): 595,\n",
              " (24, 22): 596,\n",
              " (24, 23): 597,\n",
              " (24, 25): 598,\n",
              " (24, 26): 599,\n",
              " (25, 1): 600,\n",
              " (25, 2): 601,\n",
              " (25, 3): 602,\n",
              " (25, 4): 603,\n",
              " (25, 5): 604,\n",
              " (25, 6): 605,\n",
              " (25, 7): 606,\n",
              " (25, 8): 607,\n",
              " (25, 9): 608,\n",
              " (25, 10): 609,\n",
              " (25, 11): 610,\n",
              " (25, 12): 611,\n",
              " (25, 13): 612,\n",
              " (25, 14): 613,\n",
              " (25, 15): 614,\n",
              " (25, 16): 615,\n",
              " (25, 17): 616,\n",
              " (25, 18): 617,\n",
              " (25, 19): 618,\n",
              " (25, 20): 619,\n",
              " (25, 21): 620,\n",
              " (25, 22): 621,\n",
              " (25, 23): 622,\n",
              " (25, 24): 623,\n",
              " (25, 26): 624,\n",
              " (26, 1): 625,\n",
              " (26, 2): 626,\n",
              " (26, 3): 627,\n",
              " (26, 4): 628,\n",
              " (26, 5): 629,\n",
              " (26, 6): 630,\n",
              " (26, 7): 631,\n",
              " (26, 8): 632,\n",
              " (26, 9): 633,\n",
              " (26, 10): 634,\n",
              " (26, 11): 635,\n",
              " (26, 12): 636,\n",
              " (26, 13): 637,\n",
              " (26, 14): 638,\n",
              " (26, 15): 639,\n",
              " (26, 16): 640,\n",
              " (26, 17): 641,\n",
              " (26, 18): 642,\n",
              " (26, 19): 643,\n",
              " (26, 20): 644,\n",
              " (26, 21): 645,\n",
              " (26, 22): 646,\n",
              " (26, 23): 647,\n",
              " (26, 24): 648,\n",
              " (26, 25): 649}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pair_ix = {pair:i for i,pair in enumerate(all_pairs)}\n",
        "pair_ix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UGor1EoC8uU5"
      },
      "outputs": [],
      "source": [
        "def make_negatives(possible_pairs, data, random_state, negative_only = True):\n",
        "  \"\"\"\n",
        "  possible_pairs is the total permutations possible between SenderID and RecieverID\n",
        "  data is the dataset for a given timestamp\n",
        "  random_state sets the random seed using np.random.seed()\n",
        "  \"\"\"\n",
        "\n",
        "  #Random Seed\n",
        "  np.random.seed(random_state)\n",
        "  rng = np.random.default_rng()\n",
        "\n",
        "  #Remapping the data into a list\n",
        "  known_pairs = list(map(set,data[['SenderID','ReceiverID']].values))\n",
        "  len_known_pairs = len(known_pairs)\n",
        "\n",
        "\n",
        "  #Manually checking if the pair exists\n",
        "  arr = []\n",
        "  for i in possible_pairs:\n",
        "    found = False\n",
        "    for j in known_pairs:\n",
        "      if i==j:\n",
        "        found = True\n",
        "        break\n",
        "\n",
        "    if found:\n",
        "      arr.append(True)\n",
        "    else:\n",
        "      arr.append(False)\n",
        "\n",
        "  arr = np.array(arr)\n",
        "\n",
        "  #Filtering the leftover pairs and randomly choosing from them an equal number of negatives as positives\n",
        "  leftover_pairs = pd.DataFrame(possible_pairs, columns=['SenderID','ReceiverID'])[~arr]\n",
        "  rng.shuffle(leftover_pairs.values)\n",
        "  chosen = leftover_pairs[:len_known_pairs].reset_index(drop=True)\n",
        "\n",
        "  #Creating a deep copy and updating the ID pairs to the random leftovers chosen\n",
        "  copy = data.copy(deep=True).reset_index(drop=True)\n",
        "  copy[\"SenderID\"], copy[\"ReceiverID\"], copy[\"present\"] = chosen[\"SenderID\"], chosen[\"ReceiverID\"], 0\n",
        "  # print(copy[['SenderID','ReceiverID','present']], 'Length:', len(copy), len_known_pairs)\n",
        "\n",
        "  if negative_only:\n",
        "    return copy\n",
        "\n",
        "  else:\n",
        "    results = pd.concat((data.copy(deep=True), copy), axis=0).values.tolist()\n",
        "    rng.shuffle(results)\n",
        "    results = pd.DataFrame(results, columns = data.columns)\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3rDxq_uRC2T"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2RMaU6lOzA_",
        "outputId": "1413cbb8-13f3-417d-f8c7-89f2c4e81da6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "#Device Agnostic Code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u33M00xbPtCw"
      },
      "source": [
        "## Data Prep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mn5DS_YtZPWz"
      },
      "outputs": [],
      "source": [
        "def pad_input(data, size=50):\n",
        "  '''\n",
        "  size: the length that the data will be padded/shortened to; defaults to 50\n",
        "  '''\n",
        "  # print(data.shape[2])\n",
        "  features = np.zeros((1, size, data.shape[2]),dtype=float)\n",
        "  for ii, row in enumerate(data):\n",
        "      if len(row) != 0:\n",
        "          features[ii, -len(row):] = np.array(row)[:size]\n",
        "  return features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4u4m3o1ma0A"
      },
      "source": [
        "### Train test split according to timestamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMJ-eRReF5eP",
        "outputId": "c59ed889-3034-4cfc-ac7d-cd880c679f0c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([-2.10915175, -2.10761067, -2.1060696 , ...,  2.06099688,\n",
              "         2.06356538,  2.06407907]),\n",
              " (2233,))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Getting all unique timestamps\n",
        "timestamps = np.unique(scaled_data['Timestamp'])\n",
        "timestamps, timestamps.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6a6GIh3IwiX",
        "outputId": "dd9fbff8-8a43-4ef6-baf2-7a02dd97d3f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((1786,), (447,))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Splitting the unique timestamps to a 8:2 ratio\n",
        "train_size = 0.8\n",
        "train_timestamps = timestamps[:round(len(timestamps)*train_size)]\n",
        "test_timestamps = timestamps[round(len(timestamps)*train_size): ]\n",
        "train_timestamps.shape, test_timestamps.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6RL6pKUEuWU",
        "outputId": "566e2bce-f55d-49d4-ced0-0c28da808523"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Getting max_size for pad_input function\n",
        "shapes = []\n",
        "for timestamp in timestamps:\n",
        "  current = scaled_data[scaled_data['Timestamp'] == timestamp]\n",
        "  shapes.append(current.shape[0])\n",
        "\n",
        "max_size = max(shapes)*2 # x2 because we have to make equal number negative as positives\n",
        "max_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv1JklPmn4No"
      },
      "source": [
        "Train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "jGTNMsdyJCr8",
        "outputId": "9219e8fc-fcff-4e3e-ebec-67b16bbed779"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\linco\\AppData\\Local\\Temp\\ipykernel_29684\\1980275651.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  timestamp_train_data['present'] = 1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SenderID</th>\n",
              "      <th>ReceiverID</th>\n",
              "      <th>Receiver_XVelocity</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Pathloss</th>\n",
              "      <th>PropDelay</th>\n",
              "      <th>Distance</th>\n",
              "      <th>Sender_XVelocity</th>\n",
              "      <th>Sender_YVelocity</th>\n",
              "      <th>Receiver_YVelocity</th>\n",
              "      <th>present</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>-1.272443</td>\n",
              "      <td>-1.232738</td>\n",
              "      <td>-1.232222</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>-1.427426</td>\n",
              "      <td>-1.264487</td>\n",
              "      <td>-1.264923</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>-1.175571</td>\n",
              "      <td>-1.207890</td>\n",
              "      <td>-1.208487</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>-1.351286</td>\n",
              "      <td>-1.249303</td>\n",
              "      <td>-1.249612</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>-0.963413</td>\n",
              "      <td>-1.145772</td>\n",
              "      <td>-1.145856</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39803</th>\n",
              "      <td>1</td>\n",
              "      <td>22</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>0.925225</td>\n",
              "      <td>0.764216</td>\n",
              "      <td>0.773003</td>\n",
              "      <td>0.773631</td>\n",
              "      <td>1.035886</td>\n",
              "      <td>1.005340</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39804</th>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>0.925225</td>\n",
              "      <td>0.713556</td>\n",
              "      <td>0.644625</td>\n",
              "      <td>0.644631</td>\n",
              "      <td>1.035886</td>\n",
              "      <td>1.005340</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39805</th>\n",
              "      <td>1</td>\n",
              "      <td>24</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>0.925225</td>\n",
              "      <td>0.800216</td>\n",
              "      <td>0.869632</td>\n",
              "      <td>0.870132</td>\n",
              "      <td>1.035886</td>\n",
              "      <td>1.005340</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39806</th>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>0.925225</td>\n",
              "      <td>0.747799</td>\n",
              "      <td>0.731591</td>\n",
              "      <td>0.730976</td>\n",
              "      <td>1.035886</td>\n",
              "      <td>1.005340</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39807</th>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>0.925225</td>\n",
              "      <td>0.761552</td>\n",
              "      <td>0.766101</td>\n",
              "      <td>0.766659</td>\n",
              "      <td>1.035886</td>\n",
              "      <td>1.005340</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>39808 rows Ã— 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       SenderID  ReceiverID  Receiver_XVelocity  Timestamp  Pathloss  \\\n",
              "0             1           2           -0.415269  -2.109152 -1.272443   \n",
              "1             1           3           -0.415269  -2.109152 -1.427426   \n",
              "2             1           4           -0.415269  -2.109152 -1.175571   \n",
              "3             1           5           -0.415269  -2.109152 -1.351286   \n",
              "4             1           6           -0.415269  -2.109152 -0.963413   \n",
              "...         ...         ...                 ...        ...       ...   \n",
              "39803         1          22           -0.415269   0.925225  0.764216   \n",
              "39804         1          23           -0.415269   0.925225  0.713556   \n",
              "39805         1          24           -0.415269   0.925225  0.800216   \n",
              "39806         1          25           -0.415269   0.925225  0.747799   \n",
              "39807         1          26           -0.415269   0.925225  0.761552   \n",
              "\n",
              "       PropDelay  Distance  Sender_XVelocity  Sender_YVelocity  \\\n",
              "0      -1.232738 -1.232222         -0.575348         -0.725865   \n",
              "1      -1.264487 -1.264923         -0.575348         -0.725865   \n",
              "2      -1.207890 -1.208487         -0.575348         -0.725865   \n",
              "3      -1.249303 -1.249612         -0.575348         -0.725865   \n",
              "4      -1.145772 -1.145856         -0.575348         -0.725865   \n",
              "...          ...       ...               ...               ...   \n",
              "39803   0.773003  0.773631          1.035886          1.005340   \n",
              "39804   0.644625  0.644631          1.035886          1.005340   \n",
              "39805   0.869632  0.870132          1.035886          1.005340   \n",
              "39806   0.731591  0.730976          1.035886          1.005340   \n",
              "39807   0.766101  0.766659          1.035886          1.005340   \n",
              "\n",
              "       Receiver_YVelocity  present  \n",
              "0               -0.550458        1  \n",
              "1               -0.550458        1  \n",
              "2               -0.550458        1  \n",
              "3               -0.550458        1  \n",
              "4               -0.550458        1  \n",
              "...                   ...      ...  \n",
              "39803           -0.550458        1  \n",
              "39804           -0.550458        1  \n",
              "39805           -0.550458        1  \n",
              "39806           -0.550458        1  \n",
              "39807           -0.550458        1  \n",
              "\n",
              "[39808 rows x 11 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "timestamp_train_data = scaled_data[np.isin(scaled_data['Timestamp'], train_timestamps)]\n",
        "timestamp_train_data['present'] = 1\n",
        "timestamp_train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "532StuMg_8ww",
        "outputId": "9e233004-1c03-4dce-ffd0-c7b63bc05a7c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SenderID</th>\n",
              "      <th>ReceiverID</th>\n",
              "      <th>Receiver_XVelocity</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Pathloss</th>\n",
              "      <th>PropDelay</th>\n",
              "      <th>Distance</th>\n",
              "      <th>Sender_XVelocity</th>\n",
              "      <th>Sender_YVelocity</th>\n",
              "      <th>Receiver_YVelocity</th>\n",
              "      <th>present</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>-1.092752</td>\n",
              "      <td>-1.185804</td>\n",
              "      <td>-1.185906</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>19.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>0.417217</td>\n",
              "      <td>0.027580</td>\n",
              "      <td>0.028075</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>0.919353</td>\n",
              "      <td>1.220257</td>\n",
              "      <td>1.220794</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>0.953317</td>\n",
              "      <td>1.330690</td>\n",
              "      <td>1.330222</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>-2.109152</td>\n",
              "      <td>-1.092752</td>\n",
              "      <td>-1.185804</td>\n",
              "      <td>-1.185906</td>\n",
              "      <td>-0.575348</td>\n",
              "      <td>-0.725865</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>5.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>-0.416087</td>\n",
              "      <td>0.925225</td>\n",
              "      <td>-0.178480</td>\n",
              "      <td>-0.710941</td>\n",
              "      <td>-0.711518</td>\n",
              "      <td>1.035886</td>\n",
              "      <td>1.005340</td>\n",
              "      <td>-0.544336</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>1.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>0.925225</td>\n",
              "      <td>0.719626</td>\n",
              "      <td>0.659809</td>\n",
              "      <td>0.659675</td>\n",
              "      <td>1.035886</td>\n",
              "      <td>1.005340</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>-0.416087</td>\n",
              "      <td>0.925225</td>\n",
              "      <td>-0.280303</td>\n",
              "      <td>-0.793766</td>\n",
              "      <td>-0.793231</td>\n",
              "      <td>1.035886</td>\n",
              "      <td>1.005340</td>\n",
              "      <td>-0.544336</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>0.925225</td>\n",
              "      <td>0.713556</td>\n",
              "      <td>0.644625</td>\n",
              "      <td>0.644631</td>\n",
              "      <td>1.035886</td>\n",
              "      <td>1.005340</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>0.925225</td>\n",
              "      <td>0.761552</td>\n",
              "      <td>0.766101</td>\n",
              "      <td>0.766659</td>\n",
              "      <td>1.035886</td>\n",
              "      <td>1.005340</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>79616 rows Ã— 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    SenderID  ReceiverID  Receiver_XVelocity  Timestamp  Pathloss  PropDelay  \\\n",
              "0        1.0         7.0           -0.415269  -2.109152 -1.092752  -1.185804   \n",
              "1       19.0        14.0           -0.415269  -2.109152  0.417217   0.027580   \n",
              "2       11.0         3.0           -0.415269  -2.109152  0.919353   1.220257   \n",
              "3        1.0        22.0           -0.415269  -2.109152  0.953317   1.330690   \n",
              "4       10.0        14.0           -0.415269  -2.109152 -1.092752  -1.185804   \n",
              "..       ...         ...                 ...        ...       ...        ...   \n",
              "45       5.0         6.0           -0.416087   0.925225 -0.178480  -0.710941   \n",
              "46       1.0        21.0           -0.415269   0.925225  0.719626   0.659809   \n",
              "47       1.0        11.0           -0.416087   0.925225 -0.280303  -0.793766   \n",
              "48       1.0        23.0           -0.415269   0.925225  0.713556   0.644625   \n",
              "49       1.0        26.0           -0.415269   0.925225  0.761552   0.766101   \n",
              "\n",
              "    Distance  Sender_XVelocity  Sender_YVelocity  Receiver_YVelocity  present  \n",
              "0  -1.185906         -0.575348         -0.725865           -0.550458      1.0  \n",
              "1   0.028075         -0.575348         -0.725865           -0.550458      0.0  \n",
              "2   1.220794         -0.575348         -0.725865           -0.550458      0.0  \n",
              "3   1.330222         -0.575348         -0.725865           -0.550458      1.0  \n",
              "4  -1.185906         -0.575348         -0.725865           -0.550458      0.0  \n",
              "..       ...               ...               ...                 ...      ...  \n",
              "45 -0.711518          1.035886          1.005340           -0.544336      0.0  \n",
              "46  0.659675          1.035886          1.005340           -0.550458      1.0  \n",
              "47 -0.793231          1.035886          1.005340           -0.544336      1.0  \n",
              "48  0.644631          1.035886          1.005340           -0.550458      1.0  \n",
              "49  0.766659          1.035886          1.005340           -0.550458      1.0  \n",
              "\n",
              "[79616 rows x 11 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_timestamp_train_data = pd.DataFrame([], columns= timestamp_train_data.columns)\n",
        "for timestamp in train_timestamps:\n",
        "  current_timestamp = timestamp_train_data[timestamp_train_data['Timestamp'] == timestamp].copy(deep=True).reset_index(drop=True)\n",
        "  new_timestamp_train_data = pd.concat((new_timestamp_train_data,  make_negatives(possible_pairs, current_timestamp, random_state=42, negative_only=False)), axis=0)\n",
        "\n",
        "new_timestamp_train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Y-aPmHEyiJK3"
      },
      "outputs": [],
      "source": [
        "def make_formatted_tensor(timestamps, data, max_size=max_size, lookback=5):\n",
        "  X = {0:[],1:[],2:[],3:[],4:[]}\n",
        "  y = {0:[],1:[],2:[],3:[],4:[]}\n",
        "\n",
        "  # Seperating the data according to the timestamp\n",
        "  for i in range(len(timestamps)-lookback):\n",
        "\n",
        "    # Creating X within timestamp window (of size lookback)\n",
        "    X_window = timestamps[i:i+lookback]\n",
        "\n",
        "    for i2, ts in enumerate(X_window):\n",
        "      current = data[data['Timestamp'] == ts].values\n",
        "\n",
        "      # Reshaping the feature\n",
        "      feature = torch.from_numpy(current[:,:-1]).unsqueeze(dim=0)\n",
        "\n",
        "      # Padding\n",
        "      feature = torch.Tensor(pad_input(feature,max_size))\n",
        "\n",
        "      if i==0:\n",
        "        X[i2]= feature\n",
        "\n",
        "      else:\n",
        "        X[i2] = torch.cat((X[i2], feature))\n",
        "\n",
        "    # Creating y with next timestep (until loopback + 1)\n",
        "    y_window = timestamps[i+1:i+lookback+1]\n",
        "\n",
        "    for i3, ts in enumerate(y_window):\n",
        "      current = data[data['Timestamp'] == ts].values\n",
        "\n",
        "      # Reshaping the target and to also contain not just the present but also the sender and receiver ID pair\n",
        "      target = torch.from_numpy(current[:,[0,1,-1]]).unsqueeze(dim=0)\n",
        "\n",
        "      # Padding\n",
        "      target = torch.Tensor(pad_input(target,max_size))\n",
        "\n",
        "      if i==0:\n",
        "        y[i3]= target\n",
        "\n",
        "      else:\n",
        "        y[i3] = torch.cat((y[i3], target))\n",
        "\n",
        "  # Converting dictionary to Tensor\n",
        "  for i4 in range(len(X)):\n",
        "    col_X = X[i4].unsqueeze(dim=0)\n",
        "    col_y = y[i4].unsqueeze(dim=0)\n",
        "    if i4 == 0:\n",
        "      new_X, new_y = col_X, col_y\n",
        "      continue\n",
        "\n",
        "    new_X = torch.cat((new_X,col_X))\n",
        "    new_y = torch.cat((new_y,col_y))\n",
        "\n",
        "  # Reshapping the tensor\n",
        "  new_X = new_X.reshape(len(X),len(X[0]),-1)\n",
        "  new_X = torch.transpose(new_X, 0,1)\n",
        "\n",
        "  new_y = new_y.reshape(len(y),len(y[0]),-1)\n",
        "  new_y = torch.transpose(new_y, 0,1)\n",
        "\n",
        "  return new_X, new_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "NPkGr_YWwTTT",
        "outputId": "1fe5b10c-5d9e-44bb-f139-83ac3894654d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505]],\n",
              "\n",
              "        [[ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505]],\n",
              "\n",
              "        [[ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259, -0.5505]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  1.0359,  1.0053, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186]],\n",
              "\n",
              "        [[ 0.0000,  0.0000,  0.0000,  ...,  1.0359,  1.0053, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  1.0359,  1.0053, -0.5505]],\n",
              "\n",
              "        [[ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ...,  1.0359,  1.0053, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186]]])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([1781, 5, 1500])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_X_train, new_y_train = make_formatted_tensor(train_timestamps, new_timestamp_train_data)\n",
        "display(new_X_train)\n",
        "new_X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "Fhog4J4Q3AwB",
        "outputId": "fe0b6ed5-7581-4235-af98-63cac1765f95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.,  0.,  0.,  ..., 20.,  6.,  0.],\n",
              "         [ 0.,  0.,  0.,  ..., 26.,  6.,  0.],\n",
              "         [ 0.,  0.,  0.,  ..., 14., 10.,  1.],\n",
              "         [ 0.,  0.,  0.,  ...,  9., 20.,  0.],\n",
              "         [ 0.,  0.,  0.,  ...,  1.,  5.,  0.]],\n",
              "\n",
              "        [[ 0.,  0.,  0.,  ..., 26.,  6.,  0.],\n",
              "         [ 0.,  0.,  0.,  ..., 14., 10.,  1.],\n",
              "         [ 0.,  0.,  0.,  ...,  9., 20.,  0.],\n",
              "         [ 0.,  0.,  0.,  ...,  1.,  5.,  0.],\n",
              "         [ 0.,  0.,  0.,  ...,  9., 14.,  1.]],\n",
              "\n",
              "        [[ 0.,  0.,  0.,  ..., 14., 10.,  1.],\n",
              "         [ 0.,  0.,  0.,  ...,  9., 20.,  0.],\n",
              "         [ 0.,  0.,  0.,  ...,  1.,  5.,  0.],\n",
              "         [ 0.,  0.,  0.,  ...,  9., 14.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 18., 15.,  0.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.,  0.,  0.,  ...,  2., 14.,  0.],\n",
              "         [ 0.,  0.,  0.,  ..., 24.,  1.,  1.],\n",
              "         [ 0.,  0.,  0.,  ...,  2.,  5.,  0.],\n",
              "         [ 0.,  0.,  0.,  ..., 10., 22.,  0.],\n",
              "         [ 0.,  0.,  0.,  ...,  8.,  2.,  0.]],\n",
              "\n",
              "        [[ 0.,  0.,  0.,  ..., 24.,  1.,  1.],\n",
              "         [ 0.,  0.,  0.,  ...,  2.,  5.,  0.],\n",
              "         [ 0.,  0.,  0.,  ..., 10., 22.,  0.],\n",
              "         [ 0.,  0.,  0.,  ...,  8.,  2.,  0.],\n",
              "         [ 0.,  0.,  0.,  ..., 18.,  7.,  0.]],\n",
              "\n",
              "        [[ 0.,  0.,  0.,  ...,  2.,  5.,  0.],\n",
              "         [ 0.,  0.,  0.,  ..., 10., 22.,  0.],\n",
              "         [ 0.,  0.,  0.,  ...,  8.,  2.,  0.],\n",
              "         [ 0.,  0.,  0.,  ..., 18.,  7.,  0.],\n",
              "         [ 0.,  0.,  0.,  ...,  1., 26.,  1.]]])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([1781, 5, 450])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display(new_y_train)\n",
        "new_y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "LuLqOGgNVA2O",
        "outputId": "1d279afe-48eb-40c3-b639-31a766533242"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ntrain_data_seq = None\\nflag = True\\n# Seperating the data according to the timestamp\\nfor timestamp in train_timestamps:\\n  current = new_timestamp_train_data[new_timestamp_train_data['Timestamp'] == timestamp].values.tolist()\\n  current = torch.Tensor(current).unsqueeze(dim=0)\\n  current = pad_input(current, max_size) #<-- this should be the max, not an arbitarily put number <- will cause the dropout rate to be very high\\n  current = torch.Tensor(current).to(device)\\n\\n  if flag:\\n    flag = False\\n    train_data_seq = current\\n    continue\\n\\n  train_data_seq = torch.cat((train_data_seq, current), dim=0)\\n\\ntrain_data_seq\\n\\n\""
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "train_data_seq = None\n",
        "flag = True\n",
        "# Seperating the data according to the timestamp\n",
        "for timestamp in train_timestamps:\n",
        "  current = new_timestamp_train_data[new_timestamp_train_data['Timestamp'] == timestamp].values.tolist()\n",
        "  current = torch.Tensor(current).unsqueeze(dim=0)\n",
        "  current = pad_input(current, max_size) #<-- this should be the max, not an arbitarily put number <- will cause the dropout rate to be very high\n",
        "  current = torch.Tensor(current).to(device)\n",
        "\n",
        "  if flag:\n",
        "    flag = False\n",
        "    train_data_seq = current\n",
        "    continue\n",
        "\n",
        "  train_data_seq = torch.cat((train_data_seq, current), dim=0)\n",
        "\n",
        "train_data_seq\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cX_S5iPH-zls"
      },
      "outputs": [],
      "source": [
        "# train_data_seq.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzkfE62Qn09S"
      },
      "source": [
        "Test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "N_ce4ZoQV3Jn",
        "outputId": "34a0c40a-177c-4d1a-ae99-b61510892a24"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\linco\\AppData\\Local\\Temp\\ipykernel_29684\\4138776864.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  timestamp_test_data['present'] = 1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SenderID</th>\n",
              "      <th>ReceiverID</th>\n",
              "      <th>Receiver_XVelocity</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Pathloss</th>\n",
              "      <th>PropDelay</th>\n",
              "      <th>Distance</th>\n",
              "      <th>Sender_XVelocity</th>\n",
              "      <th>Sender_YVelocity</th>\n",
              "      <th>Receiver_YVelocity</th>\n",
              "      <th>present</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39808</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>1.462951</td>\n",
              "      <td>0.927279</td>\n",
              "      <td>-0.197696</td>\n",
              "      <td>-0.727506</td>\n",
              "      <td>-0.727710</td>\n",
              "      <td>-0.576049</td>\n",
              "      <td>-0.720483</td>\n",
              "      <td>1.418618</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39809</th>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>1.462951</td>\n",
              "      <td>0.927279</td>\n",
              "      <td>-0.343771</td>\n",
              "      <td>-0.839320</td>\n",
              "      <td>-0.839366</td>\n",
              "      <td>-0.576049</td>\n",
              "      <td>-0.720483</td>\n",
              "      <td>1.418618</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39810</th>\n",
              "      <td>14</td>\n",
              "      <td>3</td>\n",
              "      <td>1.462951</td>\n",
              "      <td>0.927279</td>\n",
              "      <td>-0.406187</td>\n",
              "      <td>-0.882113</td>\n",
              "      <td>-0.881455</td>\n",
              "      <td>-0.576049</td>\n",
              "      <td>-0.720483</td>\n",
              "      <td>1.418618</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39811</th>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>1.462951</td>\n",
              "      <td>0.927279</td>\n",
              "      <td>-0.434967</td>\n",
              "      <td>-0.900058</td>\n",
              "      <td>-0.899837</td>\n",
              "      <td>-0.576049</td>\n",
              "      <td>-0.720483</td>\n",
              "      <td>1.418618</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39812</th>\n",
              "      <td>14</td>\n",
              "      <td>5</td>\n",
              "      <td>1.462951</td>\n",
              "      <td>0.927279</td>\n",
              "      <td>-0.341792</td>\n",
              "      <td>-0.837939</td>\n",
              "      <td>-0.837984</td>\n",
              "      <td>-0.576049</td>\n",
              "      <td>-0.720483</td>\n",
              "      <td>1.418618</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>14</td>\n",
              "      <td>17</td>\n",
              "      <td>-0.539460</td>\n",
              "      <td>2.064079</td>\n",
              "      <td>-1.550183</td>\n",
              "      <td>-1.286574</td>\n",
              "      <td>-1.286861</td>\n",
              "      <td>-0.681885</td>\n",
              "      <td>0.091776</td>\n",
              "      <td>0.379528</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>14</td>\n",
              "      <td>18</td>\n",
              "      <td>-0.539460</td>\n",
              "      <td>2.064079</td>\n",
              "      <td>-1.973399</td>\n",
              "      <td>-1.341790</td>\n",
              "      <td>-1.342003</td>\n",
              "      <td>-0.681885</td>\n",
              "      <td>0.091776</td>\n",
              "      <td>0.379528</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>14</td>\n",
              "      <td>19</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>2.064079</td>\n",
              "      <td>0.474808</td>\n",
              "      <td>0.131111</td>\n",
              "      <td>0.131483</td>\n",
              "      <td>-0.681885</td>\n",
              "      <td>0.091776</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>14</td>\n",
              "      <td>20</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>2.064079</td>\n",
              "      <td>0.405394</td>\n",
              "      <td>0.008254</td>\n",
              "      <td>0.007707</td>\n",
              "      <td>-0.681885</td>\n",
              "      <td>0.091776</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>14</td>\n",
              "      <td>21</td>\n",
              "      <td>-0.415269</td>\n",
              "      <td>2.064079</td>\n",
              "      <td>0.389161</td>\n",
              "      <td>-0.019354</td>\n",
              "      <td>-0.019791</td>\n",
              "      <td>-0.681885</td>\n",
              "      <td>0.091776</td>\n",
              "      <td>-0.550458</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10192 rows Ã— 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       SenderID  ReceiverID  Receiver_XVelocity  Timestamp  Pathloss  \\\n",
              "39808        14           1            1.462951   0.927279 -0.197696   \n",
              "39809        14           2            1.462951   0.927279 -0.343771   \n",
              "39810        14           3            1.462951   0.927279 -0.406187   \n",
              "39811        14           4            1.462951   0.927279 -0.434967   \n",
              "39812        14           5            1.462951   0.927279 -0.341792   \n",
              "...         ...         ...                 ...        ...       ...   \n",
              "49995        14          17           -0.539460   2.064079 -1.550183   \n",
              "49996        14          18           -0.539460   2.064079 -1.973399   \n",
              "49997        14          19           -0.415269   2.064079  0.474808   \n",
              "49998        14          20           -0.415269   2.064079  0.405394   \n",
              "49999        14          21           -0.415269   2.064079  0.389161   \n",
              "\n",
              "       PropDelay  Distance  Sender_XVelocity  Sender_YVelocity  \\\n",
              "39808  -0.727506 -0.727710         -0.576049         -0.720483   \n",
              "39809  -0.839320 -0.839366         -0.576049         -0.720483   \n",
              "39810  -0.882113 -0.881455         -0.576049         -0.720483   \n",
              "39811  -0.900058 -0.899837         -0.576049         -0.720483   \n",
              "39812  -0.837939 -0.837984         -0.576049         -0.720483   \n",
              "...          ...       ...               ...               ...   \n",
              "49995  -1.286574 -1.286861         -0.681885          0.091776   \n",
              "49996  -1.341790 -1.342003         -0.681885          0.091776   \n",
              "49997   0.131111  0.131483         -0.681885          0.091776   \n",
              "49998   0.008254  0.007707         -0.681885          0.091776   \n",
              "49999  -0.019354 -0.019791         -0.681885          0.091776   \n",
              "\n",
              "       Receiver_YVelocity  present  \n",
              "39808            1.418618        1  \n",
              "39809            1.418618        1  \n",
              "39810            1.418618        1  \n",
              "39811            1.418618        1  \n",
              "39812            1.418618        1  \n",
              "...                   ...      ...  \n",
              "49995            0.379528        1  \n",
              "49996            0.379528        1  \n",
              "49997           -0.550458        1  \n",
              "49998           -0.550458        1  \n",
              "49999           -0.550458        1  \n",
              "\n",
              "[10192 rows x 11 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "timestamp_test_data = scaled_data[np.isin(scaled_data['Timestamp'], test_timestamps)]\n",
        "timestamp_test_data['present'] = 1\n",
        "timestamp_test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "nVZPaW7B4-47",
        "outputId": "ccf9ba9d-3caa-4bbb-c664-0d3228782552"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.0000,  0.0000,  0.0000,  ..., -0.5760, -0.7205,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5760, -0.7205,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5760, -0.7205,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5760, -0.7205,  1.4186]],\n",
              "\n",
              "        [[ 0.0000,  0.0000,  0.0000,  ..., -0.5760, -0.7205,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5760, -0.7205,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5760, -0.7205,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5760, -0.7205,  1.4186]],\n",
              "\n",
              "        [[ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5760, -0.7205,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5760, -0.7205,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5760, -0.7205,  1.4186],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  1.4186]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  0.3795],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  0.3795],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  0.3795],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.6819,  0.0918, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.6819,  0.0918, -0.5505]],\n",
              "\n",
              "        [[ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  0.3795],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  0.3795],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.6819,  0.0918, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.6819,  0.0918, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.6819,  0.0918, -0.5505]],\n",
              "\n",
              "        [[ 0.0000,  0.0000,  0.0000,  ..., -0.5753, -0.7259,  0.3795],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.6819,  0.0918, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.6819,  0.0918, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.6819,  0.0918, -0.5505],\n",
              "         [ 0.0000,  0.0000,  0.0000,  ..., -0.6819,  0.0918, -0.5505]]])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([442, 5, 1500])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_X_test, new_y_test = make_formatted_tensor(test_timestamps, timestamp_test_data)\n",
        "display(new_X_test)\n",
        "new_X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "kK9etU8tMnkB",
        "outputId": "603156e5-3fd4-40e5-e803-97de250eb814"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.,  0.,  0.,  ..., 15.,  9.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 20.,  9.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 17.,  9.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 18.,  9.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 10.,  9.,  1.]],\n",
              "\n",
              "        [[ 0.,  0.,  0.,  ..., 20.,  9.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 17.,  9.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 18.,  9.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 10.,  9.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 24.,  9.,  1.]],\n",
              "\n",
              "        [[ 0.,  0.,  0.,  ..., 17.,  9.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 18.,  9.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 10.,  9.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 24.,  9.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 25.,  9.,  1.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.,  0.,  0.,  ..., 25., 18.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 26., 18.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 16., 26.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 10., 26.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 12., 26.,  1.]],\n",
              "\n",
              "        [[ 0.,  0.,  0.,  ..., 26., 18.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 16., 26.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 10., 26.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 12., 26.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 13., 26.,  1.]],\n",
              "\n",
              "        [[ 0.,  0.,  0.,  ..., 16., 26.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 10., 26.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 12., 26.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 13., 26.,  1.],\n",
              "         [ 0.,  0.,  0.,  ..., 14., 21.,  1.]]])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([442, 5, 450])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "display(new_y_test)\n",
        "new_y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "fiU2w_kaZa7t",
        "outputId": "31db2740-59a9-4522-f0d7-b43e2382427f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\ntest_data_seq = None\\nflag = True\\n# Seperating the data according to the timestamp\\nfor timestamp in test_timestamps:\\n  current = timestamp_test_data[timestamp_test_data['Timestamp'] == timestamp].values.tolist()\\n  current = torch.Tensor(current).unsqueeze(dim=0)\\n  current = pad_input(current, max_size)\\n  current = torch.Tensor(current).to(device)\\n\\n  if flag:\\n    flag = False\\n    test_data_seq = current\\n    continue\\n\\n  test_data_seq = torch.cat((test_data_seq, current), dim=0)\\n\\ntest_data_seq\\n\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "test_data_seq = None\n",
        "flag = True\n",
        "# Seperating the data according to the timestamp\n",
        "for timestamp in test_timestamps:\n",
        "  current = timestamp_test_data[timestamp_test_data['Timestamp'] == timestamp].values.tolist()\n",
        "  current = torch.Tensor(current).unsqueeze(dim=0)\n",
        "  current = pad_input(current, max_size)\n",
        "  current = torch.Tensor(current).to(device)\n",
        "\n",
        "  if flag:\n",
        "    flag = False\n",
        "    test_data_seq = current\n",
        "    continue\n",
        "\n",
        "  test_data_seq = torch.cat((test_data_seq, current), dim=0)\n",
        "\n",
        "test_data_seq\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XU99dcseiSHe"
      },
      "outputs": [],
      "source": [
        "# test_data_seq.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y5KC0BumopR"
      },
      "source": [
        "### Creating final dataset with lookback windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "E3vHqAHKPSGe",
        "outputId": "44b78ef1-e943-4b9e-cdc8-2cf25cb56b95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\ndef create_dataset(dataset, lookback):\\n    X, y = None, None\\n    for i in range(len(dataset)-lookback):\\n        feature = dataset[:,:,:-1][i:i+lookback]\\n        target = dataset[:,:,-1].unsqueeze(dim=2)[i+1:i+lookback+1]\\n        if i == 0:\\n          X,y  = feature, target\\n          continue\\n\\n        X = torch.cat((X,feature), dim=0)\\n        y = torch.cat((y,target), dim=0)\\n    return X,y\\n\\n# Seperate the data such that each tensor contains 5 timestamps\\nlookback = 5\\nX_train, y_train = create_dataset(train_data_seq, lookback=lookback)\\nX_test, y_test = create_dataset(test_data_seq, lookback=lookback)\\nprint(X_train.shape, y_train.shape)\\nprint(X_test.shape, y_test.shape)\\n'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "def create_dataset(dataset, lookback):\n",
        "    X, y = None, None\n",
        "    for i in range(len(dataset)-lookback):\n",
        "        feature = dataset[:,:,:-1][i:i+lookback]\n",
        "        target = dataset[:,:,-1].unsqueeze(dim=2)[i+1:i+lookback+1]\n",
        "        if i == 0:\n",
        "          X,y  = feature, target\n",
        "          continue\n",
        "\n",
        "        X = torch.cat((X,feature), dim=0)\n",
        "        y = torch.cat((y,target), dim=0)\n",
        "    return X,y\n",
        "\n",
        "# Seperate the data such that each tensor contains 5 timestamps\n",
        "lookback = 5\n",
        "X_train, y_train = create_dataset(train_data_seq, lookback=lookback)\n",
        "X_test, y_test = create_dataset(test_data_seq, lookback=lookback)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Ny35GJ3Wv0vA"
      },
      "outputs": [],
      "source": [
        "# X_train[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "058G-SIYhz_V"
      },
      "outputs": [],
      "source": [
        "# X_train[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLd0wKtsdg2v"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "suKkNyOQREtp"
      },
      "outputs": [],
      "source": [
        "#Hyper Parameters\n",
        "input_size = new_X_train.shape[-1]\n",
        "nodes = len(pair_ix)\n",
        "num_layers = 1\n",
        "sequence_length = 5\n",
        "\n",
        "hidden_size = 16\n",
        "#num_classes = 1\n",
        "\n",
        "num_epochs = 600 #1000\n",
        "batch_size = 64\n",
        "learning_rate = 0.00001 #0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JKboJqRTVwz",
        "outputId": "8575e817-a01f-4373-e634-b37d99249427"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x276dd26b590>"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "wJc71ycDO6i8"
      },
      "outputs": [],
      "source": [
        "class LSTMEncoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, nodes):\n",
        "    super(LSTMEncoder, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size, nodes*hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "      _, (hn,cn) = self.lstm(x)\n",
        "      # hn -> (num_layers, batch_size, hidden_size)\n",
        "      # hn -> 1, 64, 16*nodes\n",
        "\n",
        "      return (hn,cn)\n",
        "\n",
        "class LSTMDecoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(LSTMDecoder, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=False)\n",
        "    self.fc = nn.Sequential( #supposed to be MLP\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, 1),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "      out, _ = self.lstm(x)\n",
        "      # x -> (seq_length, batch_size, input_size ->[node*hidden_size])\n",
        "\n",
        "      # out: (seq_length, batch_size, hidden_size) # from lstm\n",
        "      out = self.fc(out)\n",
        "      out = out[-1,:,:]\n",
        "\n",
        "      return out\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, nodes, batch_size):\n",
        "    super(Seq2Seq, self).__init__()\n",
        "\n",
        "    self.encoder = LSTMEncoder(input_size, hidden_size, num_layers, nodes).to(device)\n",
        "    self.decoder = LSTMDecoder(batch_size*hidden_size, 64, num_layers).to(device)\n",
        "\n",
        "\n",
        "  def forward(self, x, pair_idxs):\n",
        "      (hn,cn) = self.encoder(x)\n",
        "      # Reshape hn to (node, batch, hidden_size)\n",
        "      hn = hn.reshape(nodes, -1, hidden_size)\n",
        "      hn = hn.detach().cpu().numpy()\n",
        "      # Selecting the idxs of the pairs\n",
        "      hn = torch.from_numpy(hn[pair_idxs]).to(device) # hn -> (batch, batch, hidden_size)\n",
        "\n",
        "      # Reshape hn to (1, batch, batch*hidden_size)\n",
        "      batch_size = hn.shape[0]\n",
        "      hn = hn.reshape(1, batch_size, -1)\n",
        "\n",
        "      # print(hn.shape)\n",
        "      out = self.decoder(hn)\n",
        "\n",
        "      return out\n",
        "\n",
        "model = Seq2Seq(input_size, hidden_size, num_layers, nodes, batch_size).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Z9j7qXTyO9Y2"
      },
      "outputs": [],
      "source": [
        "#loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(new_X_train,new_y_train), batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(new_X_test,new_y_test), batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4zC8MWUXH-Hn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/600, step 1/28, loss = 27.47442626953125\n",
            "Epoch 1/600, step 2/28, loss = 27.115795135498047\n",
            "Epoch 1/600, step 3/28, loss = 19.574237823486328\n",
            "Epoch 1/600, step 4/28, loss = 30.436294555664062\n",
            "Epoch 1/600, step 5/28, loss = 38.165374755859375\n",
            "Epoch 1/600, step 6/28, loss = 35.270050048828125\n",
            "Epoch 1/600, step 7/28, loss = 31.70663070678711\n",
            "Epoch 1/600, step 8/28, loss = 26.92227554321289\n",
            "Epoch 1/600, step 9/28, loss = 21.058420181274414\n",
            "Epoch 1/600, step 10/28, loss = 28.967071533203125\n",
            "Epoch 1/600, step 11/28, loss = 30.170652389526367\n",
            "Epoch 1/600, step 12/28, loss = 22.690200805664062\n",
            "Epoch 1/600, step 13/28, loss = 25.6866512298584\n",
            "Epoch 1/600, step 14/28, loss = 35.16368865966797\n",
            "Epoch 1/600, step 15/28, loss = 16.597766876220703\n",
            "Epoch 1/600, step 16/28, loss = 22.609539031982422\n",
            "Epoch 1/600, step 17/28, loss = 38.16444396972656\n",
            "Epoch 1/600, step 18/28, loss = 36.572021484375\n",
            "Epoch 1/600, step 19/28, loss = 36.428436279296875\n",
            "Epoch 1/600, step 20/28, loss = 28.94521713256836\n",
            "Epoch 1/600, step 21/28, loss = 16.552650451660156\n",
            "Epoch 1/600, step 22/28, loss = 21.127246856689453\n",
            "Epoch 1/600, step 23/28, loss = 20.942182540893555\n",
            "Epoch 1/600, step 24/28, loss = 21.386680603027344\n",
            "Epoch 1/600, step 25/28, loss = 24.144926071166992\n",
            "Epoch 1/600, step 26/28, loss = 35.15803146362305\n",
            "Epoch 1/600, step 27/28, loss = 31.94233512878418\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "shape '[1, 64, -1]' is invalid for input of size 44944",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\linco\\OneDrive\\Desktop\\SP\\Yr2 Sem1\\DENG\\CA2\\LearningLSTM\\LSTM_Lincoln.ipynb Cell 42\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/linco/OneDrive/Desktop/SP/Yr2%20Sem1/DENG/CA2/LearningLSTM/LSTM_Lincoln.ipynb#X56sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m pair \u001b[39m=\u001b[39m [pair_ix[(x[\u001b[39m0\u001b[39m],x[\u001b[39m1\u001b[39m])] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m y_batch[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/linco/OneDrive/Desktop/SP/Yr2%20Sem1/DENG/CA2/LearningLSTM/LSTM_Lincoln.ipynb#X56sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# print(pair)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/linco/OneDrive/Desktop/SP/Yr2%20Sem1/DENG/CA2/LearningLSTM/LSTM_Lincoln.ipynb#X56sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(model(x_batch, pair))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/linco/OneDrive/Desktop/SP/Yr2%20Sem1/DENG/CA2/LearningLSTM/LSTM_Lincoln.ipynb#X56sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# print(outputs)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/linco/OneDrive/Desktop/SP/Yr2%20Sem1/DENG/CA2/LearningLSTM/LSTM_Lincoln.ipynb#X56sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m y_actual \u001b[39m=\u001b[39m y_batch[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[1;32mc:\\Users\\linco\\OneDrive\\Desktop\\SP\\Yr2 Sem1\\DENG\\CA2\\LearningLSTM\\LSTM_Lincoln.ipynb Cell 42\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/linco/OneDrive/Desktop/SP/Yr2%20Sem1/DENG/CA2/LearningLSTM/LSTM_Lincoln.ipynb#X56sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m hn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(hn[pair_idxs])\u001b[39m.\u001b[39mto(device) \u001b[39m# hn -> (batch, batch, hidden_size)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/linco/OneDrive/Desktop/SP/Yr2%20Sem1/DENG/CA2/LearningLSTM/LSTM_Lincoln.ipynb#X56sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# Reshape hn to (1, batch, batch*hidden_size)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/linco/OneDrive/Desktop/SP/Yr2%20Sem1/DENG/CA2/LearningLSTM/LSTM_Lincoln.ipynb#X56sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m hn \u001b[39m=\u001b[39m hn\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, batch_size, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/linco/OneDrive/Desktop/SP/Yr2%20Sem1/DENG/CA2/LearningLSTM/LSTM_Lincoln.ipynb#X56sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# print(hn.shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/linco/OneDrive/Desktop/SP/Yr2%20Sem1/DENG/CA2/LearningLSTM/LSTM_Lincoln.ipynb#X56sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(hn)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 64, -1]' is invalid for input of size 44944"
          ]
        }
      ],
      "source": [
        "#Training Loop V3\n",
        "model.train()\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(3):\n",
        "  for i, (x_batch,y_batch) in enumerate(train_loader):\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "    #forward\n",
        "    # print('x_batch before reshaping: ',x_batch.shape)\n",
        "    # x_batch = x_batch.reshape(-1, sequence_length, input_size).to(device)\n",
        "    # print('x_batch after reshaping: ', x_batch.shape)\n",
        "    y_batch = y_batch.squeeze(dim=1)\n",
        "    # print(y_batch.shape)\n",
        "    # print(y_batch)\n",
        "    pair = [pair_ix[(x[0],x[1])] for x in y_batch[:,-1,-3:-1].to(dtype=int).cpu().numpy()]\n",
        "    # print(pair)\n",
        "    outputs = torch.flatten(model(x_batch, pair))\n",
        "    # print(outputs)\n",
        "\n",
        "    y_actual = y_batch[:,-1,-1]\n",
        "    # print(outputs,y_actual,sep='\\n')\n",
        "    # print(outputs.shape, y_actual.shape)\n",
        "\n",
        "    loss = criterion(outputs, y_actual)\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item()}')\n",
        "\n",
        "    # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mtz_xfSmN5ly"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size,1)\n",
        "\n",
        "    self.s1 = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "      out,_ = self.lstm(x) # x -> (batch_size, seq, input_size)\n",
        "      # out: (batch_size, seq_length, hidden_size) # from rnn\n",
        "      # out(N, 5, 5)\n",
        "      out = out[:, -1, :]\n",
        "      out = self.fc(out)\n",
        "      out = self.s1(out)\n",
        "      return out\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gw4-M2NFPFTc"
      },
      "outputs": [],
      "source": [
        "model = RNN(input_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "#loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ9Re0CtRbYH"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train,y_train), batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_test,y_test), batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJTre4TNdkGg"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6BTpTDdTb_O",
        "outputId": "4decaeb7-7e89-4774-cb6e-16524b429e35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50/500, step 28/140, loss = 0.13984352350234985\n",
            "Epoch 50/500, step 56/140, loss = 0.09068052470684052\n",
            "Epoch 50/500, step 84/140, loss = 0.09432001411914825\n",
            "Epoch 50/500, step 112/140, loss = 0.058462969958782196\n",
            "Epoch 50/500, step 140/140, loss = 0.10584773868322372\n",
            "\n",
            "Epoch 100/500, step 28/140, loss = 0.025381287559866905\n",
            "Epoch 100/500, step 56/140, loss = 0.014111246913671494\n",
            "Epoch 100/500, step 84/140, loss = 0.009329600259661674\n",
            "Epoch 100/500, step 112/140, loss = 0.012124285101890564\n",
            "Epoch 100/500, step 140/140, loss = 0.016219327226281166\n",
            "\n",
            "Epoch 150/500, step 28/140, loss = 0.0011823135428130627\n",
            "Epoch 150/500, step 56/140, loss = 0.00039496051613241434\n",
            "Epoch 150/500, step 84/140, loss = 0.0003481428138911724\n",
            "Epoch 150/500, step 112/140, loss = 0.0005873310728929937\n",
            "Epoch 150/500, step 140/140, loss = 0.0008521536365151405\n",
            "\n",
            "Epoch 200/500, step 28/140, loss = 3.972095510107465e-05\n",
            "Epoch 200/500, step 56/140, loss = 1.2683601198659744e-05\n",
            "Epoch 200/500, step 84/140, loss = 1.5119248928385787e-05\n",
            "Epoch 200/500, step 112/140, loss = 2.5427070795558393e-05\n",
            "Epoch 200/500, step 140/140, loss = 4.1433395381318405e-05\n",
            "\n",
            "Epoch 250/500, step 28/140, loss = 0.00013799822772853076\n",
            "Epoch 250/500, step 56/140, loss = 1.719518877507653e-05\n",
            "Epoch 250/500, step 84/140, loss = 0.00010932559234788641\n",
            "Epoch 250/500, step 112/140, loss = 6.0980812122579664e-05\n",
            "Epoch 250/500, step 140/140, loss = 4.172346962150186e-06\n",
            "\n",
            "Epoch 300/500, step 28/140, loss = 2.0869163563475013e-05\n",
            "Epoch 300/500, step 56/140, loss = 4.120069661439629e-06\n",
            "Epoch 300/500, step 84/140, loss = 1.4059687600820325e-05\n",
            "Epoch 300/500, step 112/140, loss = 6.551969363499666e-06\n",
            "Epoch 300/500, step 140/140, loss = 1.2583217312567285e-06\n",
            "\n",
            "Epoch 350/500, step 28/140, loss = 2.79565006167104e-06\n",
            "Epoch 350/500, step 56/140, loss = 6.691948328807484e-07\n",
            "Epoch 350/500, step 84/140, loss = 9.94530410025618e-07\n",
            "Epoch 350/500, step 112/140, loss = 1.0403898613731144e-06\n",
            "Epoch 350/500, step 140/140, loss = 1.0861301689146785e-06\n",
            "\n",
            "Epoch 400/500, step 28/140, loss = 1.2178071528978762e-07\n",
            "Epoch 400/500, step 56/140, loss = 2.23964171652824e-08\n",
            "Epoch 400/500, step 84/140, loss = 4.30939266493624e-08\n",
            "Epoch 400/500, step 112/140, loss = 5.489100374234113e-08\n",
            "Epoch 400/500, step 140/140, loss = 5.298191396718721e-08\n",
            "\n",
            "Epoch 450/500, step 28/140, loss = 2.940217427749303e-06\n",
            "Epoch 450/500, step 56/140, loss = 1.954177264451573e-08\n",
            "Epoch 450/500, step 84/140, loss = 1.1423982471114869e-07\n",
            "Epoch 450/500, step 112/140, loss = 1.307010847995116e-06\n",
            "Epoch 450/500, step 140/140, loss = 4.106099424916465e-07\n",
            "\n",
            "Epoch 500/500, step 28/140, loss = 8.491954304190585e-07\n",
            "Epoch 500/500, step 56/140, loss = 9.627468600115208e-09\n",
            "Epoch 500/500, step 84/140, loss = 4.5527446701498775e-08\n",
            "Epoch 500/500, step 112/140, loss = 4.020382959879498e-07\n",
            "Epoch 500/500, step 140/140, loss = 2.384186643666908e-07\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Training Loop V3\n",
        "model.train()\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (x_batch,y_batch) in enumerate(train_loader):\n",
        "\n",
        "    #forward\n",
        "    # print('x_batch before reshaping: ',x_batch.shape)\n",
        "    x_batch = x_batch.reshape(-1, sequence_length, input_size).to(device)\n",
        "    # print('x_batch after reshaping: ', x_batch.shape)\n",
        "    outputs = model(x_batch)\n",
        "\n",
        "    y_batch = y_batch.squeeze(dim=1)[:,-1]\n",
        "    # print(outputs,y_batch,sep='\\n')\n",
        "    # print(outputs.shape, y_batch.shape)\n",
        "    loss = criterion(outputs, y_batch)\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch+1)%(num_epochs//10) == 0:\n",
        "      if (i+1)%(n_total_steps//5)==0:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item()}')\n",
        "\n",
        "      if (i+1)==n_total_steps:\n",
        "        print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "_TM1IfAMewH5",
        "outputId": "738c04bc-f745-442a-996d-b1140bf840e0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nmodel.train()\\nnum_timstamps = len(train_timestamps)\\nfor tsi, time in enumerate(train_timestamps): #tsi is timestamp index, time is the actual value of the timestamp\\n  # Get data for current timestamp\\n  current_train_seq = train_data_seq[tsi]\\n\\n  # Split to training features/ nodes and target (\"present\")\\n  X_train, y_train = current_train_seq[:,:,:-1].clone().detach(), current_train_seq[:,:,-1].clone().detach()\\n\\n  # Creating trainset and train_loader\\n  trainset = dataset(X_train,y_train)\\n  train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size)\\n  n_total_steps = len(train_loader)\\n\\n  for epoch in range(num_epochs):\\n    for i, (x_train,y_train) in enumerate(train_loader):\\n      #---Forward---\\n      print(x_train.shape)\\n      # x_train = x_train.reshape(1,sequence_length, -1)\\n      outputs = model(x_train)\\n      y_train = y_train[:,-1].unsqueeze(dim=1)\\n\\n      # print(outputs,y_train,sep=\\'\\n\\')\\n      # print(outputs.shape, y_train.shape)\\n\\n      loss = criterion(outputs, y_train)\\n\\n      #---Backward---\\n      optimizer.zero_grad()\\n      loss.backward()\\n      optimizer.step()\\n\\n      # Printing progress\\n      if (tsi+1)%100 == 0:\\n        if (epoch+1)%100 == 0:\\n          print(f\\'Timestamp {tsi+1}/{num_timstamps}, Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item()}\\')\\n\\n      if (epoch+1)==num_epochs:\\n        print(\\'\\')\\n'"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Training Loop V2\n",
        "'''\n",
        "model.train()\n",
        "num_timstamps = len(train_timestamps)\n",
        "for tsi, time in enumerate(train_timestamps): #tsi is timestamp index, time is the actual value of the timestamp\n",
        "  # Get data for current timestamp\n",
        "  current_train_seq = train_data_seq[tsi]\n",
        "\n",
        "  # Split to training features/ nodes and target (\"present\")\n",
        "  X_train, y_train = current_train_seq[:,:,:-1].clone().detach(), current_train_seq[:,:,-1].clone().detach()\n",
        "\n",
        "  # Creating trainset and train_loader\n",
        "  trainset = dataset(X_train,y_train)\n",
        "  train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size)\n",
        "  n_total_steps = len(train_loader)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    for i, (x_train,y_train) in enumerate(train_loader):\n",
        "      #---Forward---\n",
        "      print(x_train.shape)\n",
        "      # x_train = x_train.reshape(1,sequence_length, -1)\n",
        "      outputs = model(x_train)\n",
        "      y_train = y_train[:,-1].unsqueeze(dim=1)\n",
        "\n",
        "      # print(outputs,y_train,sep='\\n')\n",
        "      # print(outputs.shape, y_train.shape)\n",
        "\n",
        "      loss = criterion(outputs, y_train)\n",
        "\n",
        "      #---Backward---\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Printing progress\n",
        "      if (tsi+1)%100 == 0:\n",
        "        if (epoch+1)%100 == 0:\n",
        "          print(f'Timestamp {tsi+1}/{num_timstamps}, Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item()}')\n",
        "\n",
        "      if (epoch+1)==num_epochs:\n",
        "        print('')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCOApJZuPGD-"
      },
      "outputs": [],
      "source": [
        "# #Training Loop\n",
        "# model.train()\n",
        "\n",
        "# n_total_steps = len(train_loader)\n",
        "# for epoch in range(num_epochs):\n",
        "#   for i, (x_train,y_train) in enumerate(train_loader):\n",
        "#     # print(f'Iter {i+1}, x_train:{x_train[:5]}, y_train:{y_train[:5]}')\n",
        "\n",
        "#     #forward\n",
        "#     # print('x_train before reshaping: ',x_train.shape)\n",
        "#     x_train = x_train.reshape(-1, sequence_length, input_size).to(device)\n",
        "#     # print('x_train after reshaping: ', x_train.shape)\n",
        "#     outputs = model(x_train)\n",
        "\n",
        "#     y_train = y_train[:,-1].unsqueeze(dim=1)\n",
        "\n",
        "#     # print(outputs,y_train,sep='\\n')\n",
        "#     # print(outputs.shape, y_train.shape)\n",
        "#     loss = criterion(outputs, y_train)\n",
        "\n",
        "#     #backward\n",
        "#     optimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     if (epoch+1)%100 == 0:\n",
        "#       print(f'Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.5f}')\n",
        "\n",
        "#       if (i+1)==n_total_steps:\n",
        "#         print('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHub9cyOdmBv"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZR-IOgrqgwA",
        "outputId": "7391d15a-8c9e-444f-ccad-7c209a9aa247"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100, 1, 128)"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.input_size, model.num_layers, model.hidden_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqmCXr_sNrbY"
      },
      "outputs": [],
      "source": [
        "def binary_threshold(x, threshold= 0.5):\n",
        "  # print(x)\n",
        "  threshold = torch.tensor([threshold]).to(device)\n",
        "  results = (x>threshold).float()*1\n",
        "  # print(results)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKeXXeAWaSjL",
        "outputId": "66344346-cda4-4937-a96d-bc30a9f916b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy = 100.000%\n"
          ]
        }
      ],
      "source": [
        "#Testing Loop for Training Data\n",
        "model.eval()\n",
        "n_correct = 0\n",
        "n_samples = 0\n",
        "with torch.inference_mode():\n",
        "  for i, (x_batch,y_batch) in enumerate(train_loader):\n",
        "    x_batch = x_batch.reshape(-1, sequence_length, input_size).to(device)\n",
        "    y_batch = y_batch.squeeze(dim=1)[:,-1].to(device)\n",
        "    outputs = model(x_batch)\n",
        "    outputs = binary_threshold(outputs)\n",
        "\n",
        "    # print(outputs.squeeze(),y_batch.squeeze())\n",
        "    # print(outputs.shape, y_batch.shape)\n",
        "\n",
        "    n_samples += y_batch.shape[0] * y_batch.shape[1]\n",
        "\n",
        "    n_correct += (outputs == y_batch).sum().item()\n",
        "    # print((outputs==y_batch).sum())\n",
        "    # print(n_correct, n_samples)\n",
        "\n",
        "acc = 100* n_correct/n_samples\n",
        "print(f'Training Accuracy = {acc:.3f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4pwzonQSBaG",
        "outputId": "0e155b67-2e0e-46ae-bb51-4edda44002e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cumulative: 49/64\n",
            "Cumulative: 76/128\n",
            "Cumulative: 97/192\n",
            "Cumulative: 146/256\n",
            "Cumulative: 193/320\n",
            "Cumulative: 228/384\n",
            "Cumulative: 251/448\n",
            "Cumulative: 302/512\n",
            "Cumulative: 330/576\n",
            "Cumulative: 353/640\n",
            "Cumulative: 396/704\n",
            "Cumulative: 419/768\n",
            "Cumulative: 470/832\n",
            "Cumulative: 517/896\n",
            "Cumulative: 543/960\n",
            "Cumulative: 588/1024\n",
            "Cumulative: 622/1088\n",
            "Cumulative: 665/1152\n",
            "Cumulative: 719/1216\n",
            "Cumulative: 748/1280\n",
            "Cumulative: 801/1344\n",
            "Cumulative: 847/1408\n",
            "Cumulative: 887/1472\n",
            "Cumulative: 931/1536\n",
            "Cumulative: 982/1600\n",
            "Cumulative: 1019/1664\n",
            "Cumulative: 1062/1728\n",
            "Cumulative: 1119/1792\n",
            "Cumulative: 1162/1856\n",
            "Cumulative: 1207/1920\n",
            "Cumulative: 1252/1984\n",
            "Cumulative: 1287/2048\n",
            "Cumulative: 1339/2112\n",
            "Cumulative: 1393/2176\n",
            "Cumulative: 1412/2210\n",
            "Testing Accuracy = 63.891%\n"
          ]
        }
      ],
      "source": [
        "#Testing Loop for Testing Data\n",
        "model.eval()\n",
        "n_correct = 0\n",
        "n_samples = 0\n",
        "with torch.inference_mode():\n",
        "  for i, (x_batch,y_batch) in enumerate(test_loader):\n",
        "    x_batch = x_batch.reshape(-1, sequence_length, input_size).to(device)\n",
        "    y_batch = y_batch.squeeze(dim=1)[:,-1].to(device)\n",
        "    outputs = model(x_batch)\n",
        "    # print(outputs)\n",
        "    outputs = binary_threshold(outputs)\n",
        "\n",
        "    # print(outputs,y_batch)\n",
        "    # print(outputs.shape, y_batch.shape)\n",
        "\n",
        "    ### If we only use the new unseen data\n",
        "    outputs = outputs[:,-1]\n",
        "    y_batch = y_batch[:,-1]\n",
        "    # print(y_batch)\n",
        "\n",
        "    # print(outputs,y_batch)\n",
        "    # print(outputs.shape, y_batch.shape)\n",
        "\n",
        "    # n_samples += y_batch.shape[0] * y_batch.shape[1]\n",
        "    n_samples += y_batch.shape[0]\n",
        "\n",
        "    n_correct += (outputs == y_batch).sum().item()\n",
        "\n",
        "    print(f'Cumulative: {n_correct}/{n_samples}')\n",
        "\n",
        "acc = 100* n_correct/n_samples\n",
        "print(f'Testing Accuracy = {acc:.3f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNHfxBx8qFYbgy/fMj38Voa",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
