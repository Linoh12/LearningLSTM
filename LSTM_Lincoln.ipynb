{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPLCpbRcyNiAasNLfMoRFi6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Linoh12/LearningLSTM/blob/main/LSTM_Lincoln.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import"
      ],
      "metadata": {
        "id": "wEe7a52QOsje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn-intelex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tr0PG0POOdf",
        "outputId": "e918d51d-6e23-4a51-8b67-41d25e344f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn-intelex\n",
            "  Downloading scikit_learn_intelex-2023.2.1-py310-none-manylinux1_x86_64.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting daal4py==2023.2.1 (from scikit-learn-intelex)\n",
            "  Downloading daal4py-2023.2.1-py310-none-manylinux1_x86_64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from scikit-learn-intelex) (1.2.2)\n",
            "Collecting daal==2023.2.1 (from daal4py==2023.2.1->scikit-learn-intelex)\n",
            "  Downloading daal-2023.2.1-py2.py3-none-manylinux1_x86_64.whl (75.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.10/dist-packages (from daal4py==2023.2.1->scikit-learn-intelex) (1.23.5)\n",
            "Requirement already satisfied: tbb==2021.* in /usr/local/lib/python3.10/dist-packages (from daal==2023.2.1->daal4py==2023.2.1->scikit-learn-intelex) (2021.10.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->scikit-learn-intelex) (3.2.0)\n",
            "Installing collected packages: daal, daal4py, scikit-learn-intelex\n",
            "Successfully installed daal-2023.2.1 daal4py-2023.2.1 scikit-learn-intelex-2023.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "F98-YnmNNsme",
        "outputId": "a9e7e441-2233-4ba8-a6b0-11a16e63cf8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.1+cu118'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split as tts\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearnex import patch_sklearn\n",
        "patch_sklearn()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fp = 'raw_data.csv'\n",
        "data = pd.read_csv(fp)\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "ivjbP5KvPPwM",
        "outputId": "aac4bdc9-0317-4fe8-a8d7-37af6578e412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-d1d2169adcfe>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'raw_data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'raw_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "timestamps = np.unique(data['Timestamp'])\n",
        "timestamps"
      ],
      "metadata": {
        "id": "GMJ-eRReF5eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_timestamps = timestamps[:5]\n",
        "test_timestamps = timestamps[1:6]\n",
        "train_timestamps, test_timestamps"
      ],
      "metadata": {
        "id": "a6a6GIh3IwiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp_train_data = data[np.isin(data['Timestamp'], train_timestamps)]\n",
        "timestamp_train_data"
      ],
      "metadata": {
        "id": "jGTNMsdyJCr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp_train_data['present'] = 1\n",
        "arr = []\n",
        "for row in timestamp_train_data.to_numpy():\n",
        "  temp = row\n",
        "  temp[0],temp[1],temp[-1] = temp[1],temp[0],0\n",
        "  arr.append(temp)\n",
        "\n",
        "timestamp_train_data = pd.concat((timestamp_train_data, pd.DataFrame(arr, columns=timestamp_train_data.columns)), axis=0)\n",
        "timestamp_train_data"
      ],
      "metadata": {
        "id": "mJQdM9vqRGpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = []\n",
        "for time in train_timestamps:\n",
        "  arr.append(timestamp_train_data[timestamp_train_data['Timestamp'] == time].values)\n",
        "\n",
        "train_data_seq = torch.Tensor(arr)\n",
        "train_data_seq"
      ],
      "metadata": {
        "id": "S9aUpZqMYcy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_seq = torch.transpose(train_data_seq, 0, 1).numpy()\n",
        "train_data_seq.shape"
      ],
      "metadata": {
        "id": "pZDd5SKCQ8z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timestamp_test_data = data[np.isin(data['Timestamp'], test_timestamps)]\n",
        "\n",
        "timestamp_test_data['present'] = 1\n",
        "arr = []\n",
        "for row in timestamp_test_data.to_numpy():\n",
        "  temp = row\n",
        "  temp[0],temp[1],temp[-1] = temp[1],temp[0],0\n",
        "  arr.append(temp)\n",
        "\n",
        "timestamp_test_data = pd.concat((timestamp_test_data, pd.DataFrame(arr, columns=timestamp_test_data.columns)), axis=0).reset_index(drop=True)\n",
        "timestamp_test_data"
      ],
      "metadata": {
        "id": "N_ce4ZoQV3Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = []\n",
        "for time in test_timestamps:\n",
        "  arr.append(timestamp_test_data[timestamp_test_data['Timestamp'] == time].values)\n",
        "\n",
        "test_data_seq = torch.Tensor(arr)\n",
        "test_data_seq"
      ],
      "metadata": {
        "id": "4VgMj9MLZphz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_seq = torch.transpose(test_data_seq, 0, 1).numpy()\n",
        "test_data_seq.shape"
      ],
      "metadata": {
        "id": "y38bV_mFZ0Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "Y3rDxq_uRC2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Device Agnostic Code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "k2RMaU6lOzA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyper Parameters\n",
        "input_size = 10\n",
        "num_layers = 5\n",
        "sequence_length = 5\n",
        "\n",
        "hidden_size = 10\n",
        "#num_classes = 1\n",
        "\n",
        "num_epochs = 2000 #3000\n",
        "batch_size = 32\n",
        "learning_rate = 0.001 #0.00025"
      ],
      "metadata": {
        "id": "suKkNyOQREtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Prep"
      ],
      "metadata": {
        "id": "u33M00xbPtCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self,x,y):\n",
        "    self.x = torch.tensor(x,dtype=torch.float32).to(device)\n",
        "    self.y = torch.tensor(y,dtype=torch.float32).to(device)\n",
        "    self.length = self.x.shape[0]\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    return self.x[idx],self.y[idx]\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "target = ['present']\n",
        "\n",
        "X_train, y_train = train_data_seq[:,:,:-1], train_data_seq[:,:,-1] #np.expand_dims(train_data_seq[:,:,-1], axis=2)\n",
        "X_test, y_test = test_data_seq[:,:,:-1], test_data_seq[:,:,-1] #np.expand_dims(test_data_seq[:,:,-1], axis=2)\n",
        "\n",
        "trainset = dataset(X_train,y_train)\n",
        "testset = dataset(X_test,y_test)"
      ],
      "metadata": {
        "id": "aB3Mss7bXtBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "c77yKLEkSZT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asdf = iter(train_loader)\n",
        "x,y = next(asdf)\n",
        "x.shape,y.shape"
      ],
      "metadata": {
        "id": "8e464n78dOZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "asdf = iter(test_loader)\n",
        "x,y = next(asdf)\n",
        "x.shape,y.shape"
      ],
      "metadata": {
        "id": "CStUpOcwY2qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "HLd0wKtsdg2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "4JKboJqRTVwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    # self.num_classes = num_classes\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "    self.fc = nn.Linear(hidden_size,1)\n",
        "\n",
        "    self.s1 = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "      # h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "      # c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "      out,_ = self.lstm(x) # x -> (batch_size, seq, input_size)\n",
        "      # out: (batch_size, seq_length, hidden_size) # from rnn\n",
        "      # out(N, 5, 5)\n",
        "      out = out[:, -1, :]\n",
        "      out = self.fc(out)\n",
        "      out = self.s1(out)\n",
        "      return out\n"
      ],
      "metadata": {
        "id": "Mtz_xfSmN5ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN(input_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "#loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "gw4-M2NFPFTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "lJTre4TNdkGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Loop\n",
        "model.train()\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (x_train,y_train) in enumerate(train_loader):\n",
        "    # print(f'Iter {i+1}, x_train:{x_train[:5]}, y_train:{y_train[:5]}')\n",
        "\n",
        "    #forward\n",
        "    # print('x_train before reshaping: ',x_train.shape)\n",
        "    x_train = x_train.reshape(-1, sequence_length, input_size).to(device)\n",
        "    # print('x_train after reshaping: ', x_train.shape)\n",
        "    outputs = model(x_train)\n",
        "\n",
        "    y_train = y_train[:,-1].unsqueeze(dim=1)\n",
        "\n",
        "    # print(outputs,y_train,sep='\\n')\n",
        "    # print(outputs.shape, y_train.shape)\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    #backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch+1)%100 == 0:\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.5f}')\n",
        "\n",
        "      if (i+1)==n_total_steps:\n",
        "        print('')"
      ],
      "metadata": {
        "id": "VCOApJZuPGD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "oHub9cyOdmBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.input_size, model.num_layers, model.hidden_size"
      ],
      "metadata": {
        "id": "kZR-IOgrqgwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_threshold(x, threshold= 0.5):\n",
        "  # print(x)\n",
        "  threshold = torch.tensor([threshold]).to(device)\n",
        "  results = (x>threshold).float()*1\n",
        "  # print(results)\n",
        "  return results"
      ],
      "metadata": {
        "id": "rqmCXr_sNrbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing Loop for Training Data\n",
        "model.eval()\n",
        "n_correct = 0\n",
        "n_samples = 0\n",
        "with torch.inference_mode():\n",
        "  for i, (x_train,y_train) in enumerate(train_loader):\n",
        "    x_train = x_train.reshape(-1, sequence_length, input_size).to(device)\n",
        "    y_train = y_train[:,-1].unsqueeze(dim=1).to(device)\n",
        "    outputs = model(x_train)\n",
        "    outputs = binary_threshold(outputs)\n",
        "\n",
        "    # print(outputs,y_train)\n",
        "    # print(outputs.shape, y_train.shape)\n",
        "\n",
        "    n_samples += y_train.shape[0] * y_train.shape[1]\n",
        "\n",
        "    n_correct += (outputs == y_train).sum().item()\n",
        "    # print((outputs==y_train).sum())\n",
        "    # print(n_correct, n_samples)\n",
        "\n",
        "acc = 100* n_correct/n_samples\n",
        "print(f'Training Accuracy = {acc:.3f}%')"
      ],
      "metadata": {
        "id": "LKeXXeAWaSjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing Loop for Testing Data\n",
        "model.eval()\n",
        "n_correct = 0\n",
        "n_samples = 0\n",
        "with torch.inference_mode():\n",
        "  for i, (x_test,y_test) in enumerate(test_loader):\n",
        "    x_test = x_test.reshape(-1, sequence_length, input_size).to(device)\n",
        "    y_test = y_test.to(device)\n",
        "    outputs = model(x_test)\n",
        "    outputs = binary_threshold(outputs)\n",
        "\n",
        "    # print(outputs,y_test)\n",
        "    # print(outputs.shape, y_test.shape)\n",
        "\n",
        "    ### If we only use the new unseen data\n",
        "    outputs = outputs[:,-1]\n",
        "    y_test = y_test[:,-1]\n",
        "\n",
        "    # print(outputs,y_test)\n",
        "    # print(outputs.shape, y_test.shape)\n",
        "\n",
        "    # n_samples += y_test.shape[0] * y_test.shape[1]\n",
        "    n_samples += y_test.shape[0]\n",
        "\n",
        "    n_correct += (outputs == y_test).sum().item()\n",
        "\n",
        "    print(f'Cumulative: {n_correct}/{n_samples}')\n",
        "\n",
        "acc = 100* n_correct/n_samples\n",
        "print(f'Testing Accuracy = {acc:.3f}%')"
      ],
      "metadata": {
        "id": "b4pwzonQSBaG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}